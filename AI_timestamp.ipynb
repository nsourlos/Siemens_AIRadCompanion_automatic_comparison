{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1cd0a071",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import dependencies\n",
    "import os\n",
    "import numpy as np\n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "import traceback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "396ec099",
   "metadata": {},
   "outputs": [],
   "source": [
    "start=time.time() #Count time to run script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f8d66c64",
   "metadata": {},
   "outputs": [],
   "source": [
    "#This is needed to get access to path set in other script. It's the path of all AI txt files with processing time and tasks\n",
    "%store -r path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "afb84a8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_patient_id(path,file): \n",
    "    'Function to get a extract from a txt file the participant_id and the information on it.'\n",
    "    \n",
    "    with open(path+'/'+file,'r') as f: #Read txt file\n",
    "        lines = f.readlines()\n",
    "        \n",
    "    for line in lines: #Loop over lines of txt file\n",
    "        if 'patientID :' in line: #If there is a patient_id in the txt file\n",
    "            \n",
    "            if len(line.split(':')[1].split(',')[0])==6: #If a participant_id with 6 digits exists                                                                \n",
    "                patient_id=line.split(':')[1].split(',')[0] #Get that id if it's a valid number\n",
    "\n",
    "            elif \"imalife_\" in line.split(':')[1].split(',')[0].lower(): #If 'imalife_' in participant's name also keep participant with that prefix\n",
    "                patient_id=line.split(':')[1].split(',')[0]\n",
    "\n",
    "    try: #To avoid errors when patient_id is not present\n",
    "        if len(patient_id)!=0: #This will given error if patient_id doesn't exists \n",
    "            pass\n",
    "    except:\n",
    "        patient_id=''\n",
    "        lines=''\n",
    "        \n",
    "    return patient_id,lines #return patient_id and txt file information for that patient as a list"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "71e91a69",
   "metadata": {},
   "source": [
    "### Participants that were sent for nodules, aorta, CACS, emphysema, cardiac fat, or vertebra measurements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7fa60dd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sort_and_list(pat_sent_for_parameter,parameter_files): \n",
    "    'Sorts participants and their corresponding files and returns them as lists'\n",
    "    \n",
    "    pats, parameter_files = zip(*sorted(zip(pat_sent_for_parameter, parameter_files))) #Sort them\n",
    "\n",
    "    #and convert them to list\n",
    "    pats=list(pats)\n",
    "    parameter_files=list(parameter_files)\n",
    "    \n",
    "    return pats, parameter_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1f3e33a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_parameter(path):\n",
    "    \n",
    "    'Gets the path of AI files with a timestamp and returns the participants and the corresponding file names in which measurements were performed.'\n",
    "    'These measurements are: nodules, aorta, CACS, vertebra, cardiac fat, emphysema'\n",
    "\n",
    "    pat_sent_for_nodules=[] #empty lists to be filled with participants that were sent to AI for nodule detection\n",
    "    nodule_files=[] #Files that correspond to the above participants\n",
    "    pat_sent_for_aorta=[] #empty lists to be filled with participants that were sent to AI for aorta measurements\n",
    "    aorta_files=[] #Files that correspond to the above participants\n",
    "    pat_sent_for_CACS=[] #empty lists to be filled with participants that were sent to AI for CACS\n",
    "    CACS_files=[] #Files that correspond to the above participants\n",
    "    pat_sent_for_vertebra=[] #empty lists to be filled with participants that were sent to AI for vertebra\n",
    "    vertebra_files=[] #Files that correspond to the above participants\n",
    "    pat_sent_for_fat=[] #empty lists to be filled with participants that were sent to AI for cardiac fat\n",
    "    fat_files=[] #Files that correspond to the above participants\n",
    "    pat_sent_for_emph=[] #empty lists to be filled with participants that were sent to AI for emphysema\n",
    "    emph_files=[] #Files that correspond to the above participants\n",
    "\n",
    "    no_measurements=0 #Keep track of the number of files without any of the above measurements\n",
    "    empty_files=0 #Keep track of empty files\n",
    "    measurements=0 #Keep track of number of files with measurements\n",
    "    pat_ignored=0 #Keep track of number of files with empty patient names\n",
    "\n",
    "    for file in tqdm(os.listdir(path)): #loop over all txt files \n",
    "      \n",
    "        try: #If we also have other files than txt (eg. zip) it will give error\n",
    "\n",
    "            with open(path+'/'+file,'r') as f: #Read each txt file\n",
    "                lines = f.readlines()\n",
    "\n",
    "            flag_nods=0 #Set a flag to be set to 1 if file was sent for nodules\n",
    "            flag_aorta=0 #Set a flag to be set to 1 if file was sent for aorta\n",
    "            flag_CACS=0 #Set a flag to be set to 1 if file was sent for CACS\n",
    "            flag_CACS_remove=0 #Set a flag to be set to 1 if file was sent for CACS and aorta - only keep aorta\n",
    "            flag_fat=0 #Set a flag to be set to 1 if file was sent for cardiac fat\n",
    "            flag_vertebra=0 #Set a flag to be set to 1 if file was sent for vertebra\n",
    "            flag_emph=0 #Set a flag to be set to 1 if file was sent for emphysema\n",
    "            \n",
    "            flag_all=0 #Set a flag to be set to 1 if file was sent to AI for any of the above\n",
    "\n",
    "            for line in lines: #Loop over all lines of txt file\n",
    "\n",
    "                if \"Slices :\" in line: #If 'slices' in one of the lines of the txt file\n",
    "                    slice_num=int(line.split('Slices : ')[-1].split(',')[0]) #Get number of slices sent to AI\n",
    "                \n",
    "                if 'LungCAD' in line: #If this string exists, then file was sent for nodules\n",
    "                    flag_nods=1\n",
    "                    \n",
    "                #we only care for 'CalciumScoreVB40' but it's the same as the one below   \n",
    "                if 'CalciumScore' in line and slice_num<200: #If this string exists, then file was sent for CACS\n",
    "                    flag_CACS=1\n",
    "\n",
    "                if 'Aorta Overview generation' in line: #If this true, then file was sent for aorta\n",
    "                    #We repeat it again since we have already checked for CACS and now we check for aorta based on order of 'if' statements\n",
    "                    #First is CACS in the txt file and then aorta. \n",
    "                    flag_CACS_remove=1     \n",
    "                    flag_aorta=1\n",
    "                    \n",
    "                if 'Vertebra Measurement algorithms' in line: #If this string exists, then file was sent for vertebra\n",
    "                    flag_vertebra=1\n",
    "\n",
    "                if 'Cardiac Fat' in line: #If this string exists, then file was sent for cardiac fat\n",
    "                    flag_fat=1\n",
    "\n",
    "                if \"LungMeasurement finished\" in line: #If this string exists, then file was sent for emphysema\n",
    "                    flag_emph=1\n",
    "                \n",
    "                    \n",
    "                #To check for files containing any of the above measurements\n",
    "                if ('LungCAD' in line or 'Aorta Overview generation' in line or 'CalciumScore' \n",
    "                    in line or 'Vertebra Measurement algorithms' in line or 'Cardiac Fat' in line \n",
    "                    or 'LungMeasurement finished' in line):\n",
    "                    \n",
    "                    flag_all=1\n",
    "\n",
    "\n",
    "            pat_name,_=get_patient_id(path,file) #Get patient_id and txt information\n",
    "\n",
    "            if pat_name.isnumeric() or 'imalife_' in pat_name.lower(): #If patient_id is a valid number (6 digits) or 'imalife_' in its name\n",
    "\n",
    "                if flag_nods==1: #If scan sent for nodules\n",
    "                    pat_sent_for_nodules.append(pat_name) #Save its id\n",
    "                    nodule_files.append(file) #Save file name\n",
    "\n",
    "                if flag_aorta==1: #sent for aorta measurements\n",
    "                    pat_sent_for_aorta.append(pat_name)\n",
    "                    aorta_files.append(file)       \n",
    "\n",
    "                if flag_CACS==1 and flag_CACS_remove==0 : #When both aorta and CACS are present, only consider aorta measurements\n",
    "                    pat_sent_for_CACS.append(pat_name)\n",
    "                    CACS_files.append(file)\n",
    "\n",
    "                if flag_vertebra==1: #sent for vertebra measurements\n",
    "                    pat_sent_for_vertebra.append(pat_name)\n",
    "                    vertebra_files.append(file)       \n",
    "\n",
    "                if flag_fat==1: #sent for cardiac fat measurements\n",
    "                    pat_sent_for_fat.append(pat_name)\n",
    "                    fat_files.append(file)   \n",
    "                    \n",
    "                if flag_emph==1: #sent for emphysema measurements\n",
    "                    pat_sent_for_emph.append(pat_name)\n",
    "                    emph_files.append(file)                     \n",
    "                \n",
    "                    \n",
    "                #If the file does not have any of the above measurements count it    \n",
    "                if flag_all==0:  \n",
    "                    no_measurements=no_measurements+1\n",
    "                else: #Same for when measurements are available\n",
    "                    measurements=measurements+1\n",
    "                    \n",
    "            else: #If participant_id in not print a valid name\n",
    "                if pat_name!='': #Other non-proper names. Some examples: ima_40, ima_41, ima_41, ima_17, ima_59, ima_17, ima_41, ima_43, ima_18\n",
    "                    #These are the results of earlier versions\n",
    "                    pat_ignored=pat_ignored+1\n",
    "                else: #if it's empty, count it\n",
    "                    empty_files=empty_files+1\n",
    "\n",
    "        except: #For files other than txt \n",
    "            print(file)\n",
    "            \n",
    "    print(\"Total number of files without any of the above measurements (nodules, aorta, CACS, vertebra, cardiac fat) is:\",no_measurements)\n",
    "    print(\"Total number of empty files is:\",empty_files)\n",
    "    print(\"Total number of files with measurements is:\",measurements)\n",
    "    print(\"Total number of files with empty patient_names:\",pat_ignored)\n",
    "    \n",
    "    try: #Confirm that no file missed - if error should be because of the existance of non-txt AI files\n",
    "        assert(len(os.listdir(path))==no_measurements+empty_files+measurements+pat_ignored)\n",
    "    except:\n",
    "        print(traceback.format_exc())\n",
    "\n",
    "    \n",
    "    return (pat_sent_for_nodules, nodule_files,\n",
    "            pat_sent_for_aorta, aorta_files,\n",
    "            pat_sent_for_CACS, CACS_files,\n",
    "            pat_sent_for_vertebra, vertebra_files,\n",
    "            pat_sent_for_fat, fat_files,\n",
    "            pat_sent_for_emph, emph_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "07d9dd8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10029/10029 [00:02<00:00, 3621.43it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of files without any of the above measurements (nodules, aorta, CACS, vertebra, cardiac fat) is: 0\n",
      "Total number of empty files is: 492\n",
      "Total number of files with measurements is: 9528\n",
      "Total number of files with empty patient_names: 9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "(pat_sent_for_nodules, nodule_files,\n",
    " pat_sent_for_aorta, aorta_files,\n",
    " pat_sent_for_CACS, CACS_files,\n",
    " pat_sent_for_vertebra, vertebra_files,\n",
    " pat_sent_for_fat, fat_files,\n",
    " pat_sent_for_emph, emph_files)=check_parameter(path=path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "99a15352",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique participants sent for nodules:  375\n",
      "All participants sent for nodules:  547\n",
      "\n",
      "\n",
      "Unique participants sent for aorta:  1580\n",
      "All participants sent for aorta:  1685\n",
      "\n",
      "\n",
      "Unique participants sent for CACS:  4965\n",
      "All participants sent for CACS:  6951\n",
      "\n",
      "\n",
      "Unique participants sent for vertebra:  1880\n",
      "All participants sent for vertebra:  2081\n",
      "\n",
      "\n",
      "Unique participants sent for cardiac fat:  1647\n",
      "All participants sent for cardiac fat:  1648\n",
      "\n",
      "\n",
      "Unique participants sent for emphysema:  1883\n",
      "All participants sent for emphysema:  2116\n"
     ]
    }
   ],
   "source": [
    "print('Unique participants sent for nodules: ',len(np.unique(pat_sent_for_nodules))) #Unique patient_ids\n",
    "print('All participants sent for nodules: ',len(pat_sent_for_nodules)) #All patient_ids - some of them exist more than once and we will keep the latest below\n",
    "print('\\n')\n",
    "\n",
    "print('Unique participants sent for aorta: ',len(np.unique(pat_sent_for_aorta))) \n",
    "print('All participants sent for aorta: ',len(pat_sent_for_aorta)) \n",
    "print('\\n')\n",
    "\n",
    "print('Unique participants sent for CACS: ',len(np.unique(pat_sent_for_CACS))) \n",
    "print('All participants sent for CACS: ',len(pat_sent_for_CACS))\n",
    "print('\\n')\n",
    "\n",
    "print('Unique participants sent for vertebra: ',len(np.unique(pat_sent_for_vertebra))) \n",
    "print('All participants sent for vertebra: ',len(pat_sent_for_vertebra))\n",
    "print('\\n')\n",
    "\n",
    "print('Unique participants sent for cardiac fat: ',len(np.unique(pat_sent_for_fat))) \n",
    "print('All participants sent for cardiac fat: ',len(pat_sent_for_fat))\n",
    "print('\\n')\n",
    "\n",
    "print('Unique participants sent for emphysema: ',len(np.unique(pat_sent_for_emph))) \n",
    "print('All participants sent for emphysema: ',len(pat_sent_for_emph))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "6483fa9e",
   "metadata": {},
   "source": [
    "### Sort participants and their respective lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a978b54d",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    pat_sent_for_nodules, nodule_files=sort_and_list(pat_sent_for_nodules, nodule_files)\n",
    "except:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1569320d",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    pat_sent_for_aorta, aorta_files=sort_and_list(pat_sent_for_aorta, aorta_files)\n",
    "except:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "29279aab",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    pat_sent_for_CACS, CACS_files=sort_and_list(pat_sent_for_CACS, CACS_files)\n",
    "except:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d3157d5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    pat_sent_for_vertebra, vertebra_files=sort_and_list(pat_sent_for_vertebra, vertebra_files)\n",
    "except:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f50ad088",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    pat_sent_for_fat, fat_files=sort_and_list(pat_sent_for_fat, fat_files)\n",
    "except:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f72be28d",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    pat_sent_for_emph, emph_files=sort_and_list(pat_sent_for_emph, emph_files)\n",
    "except:\n",
    "    pass"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "976e4cd0",
   "metadata": {},
   "source": [
    "### Create dictionaries of participant ids and their txt files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "df6b3386",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dictionary_parameter(pat_sent_for_parameter,parameter_files):\n",
    "    \n",
    "    'Combine all files corresponding to a participants in a list and create dictionary of {participants:files of them}'\n",
    "    \n",
    "    pat_file={} #Empty dictionary to store patient_ids and filenames corresponding to that patient\n",
    "    \n",
    "    for pat in np.unique(pat_sent_for_parameter): #Loop over unique patient_ids\n",
    "        pat_file[pat]=[] #Add patient_id as key and as value set an empty list to be filled below\n",
    "\n",
    "        for index in np.where(np.array(pat_sent_for_parameter)==pat)[0]: #For all indices in which we get the same patient\n",
    "            pat_file[pat].append(parameter_files[index]) #Add file name to above dictionary\n",
    "\n",
    "    return pat_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "24828057",
   "metadata": {},
   "outputs": [],
   "source": [
    "nodule_dict=create_dictionary_parameter(pat_sent_for_nodules, nodule_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f42be71a",
   "metadata": {},
   "outputs": [],
   "source": [
    "aorta_dict=create_dictionary_parameter(pat_sent_for_aorta, aorta_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5000336c",
   "metadata": {},
   "outputs": [],
   "source": [
    "CACS_dict=create_dictionary_parameter(pat_sent_for_CACS, CACS_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "efc91fa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "vertebra_dict=create_dictionary_parameter(pat_sent_for_vertebra, vertebra_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "bb95d29d",
   "metadata": {},
   "outputs": [],
   "source": [
    "fat_dict=create_dictionary_parameter(pat_sent_for_fat, fat_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1823c55e",
   "metadata": {},
   "outputs": [],
   "source": [
    "emph_dict=create_dictionary_parameter(pat_sent_for_emph, emph_files)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "8c0b8444",
   "metadata": {},
   "source": [
    "### Keep only the latest file or the file with the most slices sent to AI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "04f8b5b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def keep_one_file(path,parameter_dict):\n",
    "    \n",
    "    'Gets the path of all AI files with time duration and the dictionary with participants and all files for them'\n",
    "    'Keeps only one file for each participant, the one with the latest date or the one with the most slices'\n",
    "\n",
    "    pat_file_final={} #Keep only one file for each patient, the latest one (or the one with the most slices)\n",
    "\n",
    "    for pat,files in parameter_dict.items(): #Loop over each patient_id and its corresponding txt files\n",
    "\n",
    "        if len(files)>1: #If more than one file\n",
    "\n",
    "            slice_num_final=0 #Initialize the number of slices for this patient to 0\n",
    "            series_final='20150101' #Set a random very early date to compare and select the latest one below\n",
    "\n",
    "            for index,file in enumerate(files): #Loop over all files of this patient\n",
    "                _,info=get_patient_id(path,file) #Get information of txt file for it\n",
    "\n",
    "                for line in info: #Loop over all lines of this file\n",
    "                    if \"Slices :\" in line: #If 'slices' in one of the lines of the txt file\n",
    "                        slice_num=int(line.split('Slices : ')[-1].split(',')[0]) #Get the number of slices in that patient\n",
    "                    \n",
    "                    if \"SeriesDate :\" in line: #If series information in line\n",
    "                        series_date=str(line.split('SeriesDate :')[-1].split(',')[0]) #Get the series_id of that patient\n",
    "                    \n",
    "                #if the number of slices is bigger than those met already, and the same series (avoid comparing with repeat scans)\n",
    "                if slice_num>slice_num_final and series_date<=series_final: \n",
    "                    slice_num_final=slice_num #Keep track of it\n",
    "                    index_final=index #and of the index in which it can be found\n",
    "                    series_final=series_date #Keep track of the series date\n",
    "\n",
    "                elif slice_num==slice_num_final and series_date<=series_final: #If it's equal to one already met:\n",
    "                    before_date=files[index_final].split('.')[-2][:12] #Compare the date of the previous time it was met\n",
    "                    current_date=files[index].split('.')[-2][:12] #with the current date of the file\n",
    "\n",
    "                    if current_date>before_date: #if the current date is the latest one\n",
    "                        index_final=index #Keep this index   \n",
    "                        \n",
    "            pat_file_final[pat]=files[index_final] #Add the final patient_id and file name information to dictionary\n",
    "\n",
    "        else: #if only 1 file then keep it as is - Assumed that it's not a repeat scan\n",
    "            pat_file_final[pat]=files[0]\n",
    "\n",
    "    return pat_file_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3b8c8466",
   "metadata": {},
   "outputs": [],
   "source": [
    "nodule_dict_final=keep_one_file(path,nodule_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d51803e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "aorta_dict_final=keep_one_file(path,aorta_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "4a55782c",
   "metadata": {},
   "outputs": [],
   "source": [
    "CACS_dict_final=keep_one_file(path,CACS_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "01ec6a4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "vertebra_dict_final=keep_one_file(path,vertebra_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "46f67eaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "fat_dict_final=keep_one_file(path,fat_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "8419b608",
   "metadata": {},
   "outputs": [],
   "source": [
    "emph_dict_final=keep_one_file(path,emph_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "575d6de3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of files with aorta measurements 1580\n"
     ]
    }
   ],
   "source": [
    "print('Total number of files with aorta measurements',len(aorta_dict_final))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "9260625d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of files with CACS measurements 4965\n"
     ]
    }
   ],
   "source": [
    "print('Total number of files with CACS measurements',len(CACS_dict_final))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "a0e2a90a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of files with nodule measurements 375\n"
     ]
    }
   ],
   "source": [
    "print('Total number of files with nodule measurements',len(nodule_dict_final))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "2f04f077",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of files with vertebra measurements 1880\n"
     ]
    }
   ],
   "source": [
    "print('Total number of files with vertebra measurements',len(vertebra_dict_final))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "cb7592e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of files with cardiac fat measurements 1647\n"
     ]
    }
   ],
   "source": [
    "print('Total number of files with cardiac fat measurements',len(fat_dict_final))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "2fa3f4fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of files with emphysema measurements 1883\n"
     ]
    }
   ],
   "source": [
    "print('Total number of files with emphysema measurements',len(emph_dict_final))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "7682235b",
   "metadata": {},
   "source": [
    "### Select files only after a specific date (avoid errors with earlier versions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "2903c0c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def keep_based_on_date(parameter_dict,date):\n",
    "    \n",
    "    'For a specific parameter, keep only files that are after a specified date. This date should be defined in the format \"YYYYMM\"'\n",
    "\n",
    "    pat_file_keep={} #Empty dictionary to be filled only with files after date specified above\n",
    "\n",
    "    for pat,file in parameter_dict.items(): #Loop over patient_id and filename corresponding to this patient\n",
    "\n",
    "        if file.split('.')[-2][:6]>=date: #If the date is later than the specified date\n",
    "            pat_file_keep[pat]=file #Keep that file\n",
    "\n",
    "    return pat_file_keep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "16fa37a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#For nodules we keep based on date below (if before February 2022 we will have the same file for nodules and for CACS \n",
    "#which is wrong since they require different kernels). There are also files sent for nodules in January (21 earliest), \n",
    "#but if kept we get errors like in 100761 (not exist in Syngo.via)\n",
    "\n",
    "nodule_dict_final_date=keep_based_on_date(nodule_dict_final,'202202') #1st February 2022 onwards for nodules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "1dea53a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "aorta_dict_final_date=keep_based_on_date(aorta_dict_final,'202209') #1st September 2022 onwards for aorta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "f2f58933",
   "metadata": {},
   "outputs": [],
   "source": [
    "CACS_dict_final_date=keep_based_on_date(CACS_dict_final,'202205') #1st May 2022 onwards for CACS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "5f3fc6cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "vertebra_dict_final_date=keep_based_on_date(vertebra_dict_final,'202209') #1st September 2022 onwards for vertebra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "8d094a6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "cardiac_fat_dict_final_date=keep_based_on_date(fat_dict_final,'202209') #1st September 2022 onwards for cardiac fat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "6b59432d",
   "metadata": {},
   "outputs": [],
   "source": [
    "emph_dict_final_date=keep_based_on_date(emph_dict_final,'202209') #1st September 2022 onwards for emphysema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "5133872c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of files with CACS measurements after 1st May 2022 3651\n"
     ]
    }
   ],
   "source": [
    "print('Total number of files with CACS measurements after 1st May 2022',len(CACS_dict_final_date))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "ca3420e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of files with aorta measurements after 1st September 2022 1368\n"
     ]
    }
   ],
   "source": [
    "print('Total number of files with aorta measurements after 1st September 2022',len(aorta_dict_final_date))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "c6c2b02f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of files with nodule measurements after 1st February 2022 334\n"
     ]
    }
   ],
   "source": [
    "print('Total number of files with nodule measurements after 1st February 2022',len(nodule_dict_final_date))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "975e964e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of files with vertebra measurements after 1st September 2022 1654\n"
     ]
    }
   ],
   "source": [
    "print('Total number of files with vertebra measurements after 1st September 2022',len(vertebra_dict_final_date))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "979b3268",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of files with CACS measurements after 1st September 2022 1647\n"
     ]
    }
   ],
   "source": [
    "print('Total number of files with CACS measurements after 1st September 2022',len(cardiac_fat_dict_final_date))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "b6e604df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of files with emphysema measurements after 1st September 2022 1654\n"
     ]
    }
   ],
   "source": [
    "print('Total number of files with emphysema measurements after 1st September 2022',len(emph_dict_final_date))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "a61967eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confirm that same file with information about parameters exists\n",
    "for file in list(nodule_dict_final_date.values()):\n",
    "    if file in os.listdir(path):\n",
    "        pass\n",
    "    else:\n",
    "        print(file) #Should not have any prints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "1734fcb0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'100785': '1.2.276.0.28.3.345049594267.42.2984.20220902083623014.txt',\n",
       " '101191': '1.2.276.0.28.3.345049594267.42.8396.20220523072606014.txt',\n",
       " '101493': '1.2.276.0.28.3.345049594267.42.9296.20221027185912014.txt',\n",
       " '102236': '1.2.276.0.28.3.345049594267.42.4032.20221027191123009.txt',\n",
       " '102427': '1.2.276.0.28.3.345049594267.42.3308.20221027192131000.txt',\n",
       " '102847': '1.2.276.0.28.3.345049594267.42.6596.20221027193526014.txt',\n",
       " '105179': '1.2.276.0.28.3.345049594267.42.9904.20221027194536000.txt',\n",
       " '106103': '1.2.276.0.28.3.345049594267.42.5716.20221018064249014.txt',\n",
       " '109640': '1.2.276.0.28.3.345049594267.42.2868.20221027195542000.txt',\n",
       " '111877': '1.2.276.0.28.3.345049594267.42.476.20221027200706014.txt',\n",
       " '113137': '1.2.276.0.28.3.345049594267.42.7164.20220822163648000.txt',\n",
       " '114616': '1.2.276.0.28.3.345049594267.42.4676.20221027201807000.txt',\n",
       " '116518': '1.2.276.0.28.3.345049594267.42.5032.20221027202803000.txt',\n",
       " '117028': '1.2.276.0.28.3.345049594267.42.3212.20221027203827014.txt',\n",
       " '119319': '1.2.276.0.28.3.345049594267.42.5080.20221031085210014.txt',\n",
       " '120772': '1.2.276.0.28.3.345049594267.42.2680.20221018065843014.txt',\n",
       " '120807': '1.2.276.0.28.3.345049594267.42.10004.20221031091035014.txt',\n",
       " '120956': '1.2.276.0.28.3.345049594267.42.7880.20221031092048000.txt',\n",
       " '121046': '1.2.276.0.28.3.345049594267.42.6008.20221031093137000.txt',\n",
       " '121077': '1.2.276.0.28.3.345049594267.42.7640.20221031094410014.txt',\n",
       " '121084': '1.2.276.0.28.3.345049594267.42.8352.20221018074217014.txt',\n",
       " '121119': '1.2.276.0.28.3.345049594267.42.7572.20221031100100014.txt',\n",
       " '121157': '1.2.276.0.28.3.345049594267.42.9808.20221018075953014.txt',\n",
       " '121275': '1.2.276.0.28.3.345049594267.42.4520.20221031101844000.txt',\n",
       " '122018': '1.2.276.0.28.3.345049594267.42.8396.20221027204724000.txt',\n",
       " '123097': '1.2.276.0.28.3.345049594267.42.7592.20221027205905014.txt',\n",
       " '123295': '1.2.276.0.28.3.345049594267.42.6904.20221031102751014.txt',\n",
       " '125260': '1.2.276.0.28.3.345049594267.42.6744.20221018082713014.txt',\n",
       " '125840': '1.2.276.0.28.3.345049594267.42.5020.20221027211354000.txt',\n",
       " '127006': '1.2.276.0.28.3.345049594267.42.4928.20220527124706014.txt',\n",
       " '127013': '1.2.276.0.28.3.345049594267.42.4088.20220526112116014.txt',\n",
       " '127964': '1.2.276.0.28.3.345049594267.42.2968.20220720105356014.txt',\n",
       " '128127': '1.2.276.0.28.3.345049594267.42.4796.20220526115019000.txt',\n",
       " '128318': '1.2.276.0.28.3.345049594267.42.7268.20221018083747000.txt',\n",
       " '129106': '1.2.276.0.28.3.345049594267.42.8160.20220720124124000.txt',\n",
       " '129960': '1.2.276.0.28.3.345049594267.42.6064.20221018084740014.txt',\n",
       " '130767': '1.2.276.0.28.3.345049594267.42.9804.20221031110431000.txt',\n",
       " '130781': '1.2.276.0.28.3.345049594267.42.8060.20221027212434014.txt',\n",
       " '132121': '1.2.276.0.28.3.345049594267.42.6456.20220526111108014.txt',\n",
       " '132676': '1.2.276.0.28.3.345049594267.42.10940.20220720142025000.txt',\n",
       " '132690': '1.2.276.0.28.3.345049594267.42.3532.20220323191243014.txt',\n",
       " '133006': '1.2.276.0.28.3.345049594267.42.864.20220720100829014.txt',\n",
       " '133971': '1.2.276.0.28.3.345049594267.42.8416.20220720101955014.txt',\n",
       " '134110': '1.2.276.0.28.3.345049594267.42.6536.20220720113105000.txt',\n",
       " '134363': '1.2.276.0.28.3.345049594267.42.5984.20221031111427000.txt',\n",
       " '134752': '1.2.276.0.28.3.345049594267.42.9336.20221027215259014.txt',\n",
       " '135311': '1.2.276.0.28.3.345049594267.42.6476.20220526112738014.txt',\n",
       " '135366': '1.2.276.0.28.3.345049594267.42.5176.20220526120215000.txt',\n",
       " '135408': '1.2.276.0.28.3.345049594267.42.6772.20220323131930014.txt',\n",
       " '135526': '1.2.276.0.28.3.345049594267.42.8184.20220323172521000.txt',\n",
       " '135717': '1.2.276.0.28.3.345049594267.42.1340.20220803135200014.txt',\n",
       " '135915': '1.2.276.0.28.3.345049594267.42.9328.20220705090356014.txt',\n",
       " '135984': '1.2.276.0.28.3.345049594267.42.1240.20220822164326014.txt',\n",
       " '136012': '1.2.276.0.28.3.345049594267.42.5712.20220822165510014.txt',\n",
       " '136109': '1.2.276.0.28.3.345049594267.42.5476.20220822170039014.txt',\n",
       " '136116': '1.2.276.0.28.3.345049594267.42.11652.20220822170552014.txt',\n",
       " '136154': '1.2.276.0.28.3.345049594267.42.9404.20220705080217014.txt',\n",
       " '136161': '1.2.276.0.28.3.345049594267.42.6920.20221018063242000.txt',\n",
       " '136185': '1.2.276.0.28.3.345049594267.42.10688.20220822171636014.txt',\n",
       " '136234': '1.2.276.0.28.3.345049594267.42.6992.20220905075201014.txt',\n",
       " '136258': '1.2.276.0.28.3.345049594267.42.3236.20220325093236014.txt',\n",
       " '136307': '1.2.276.0.28.3.345049594267.42.13912.20220822172213000.txt',\n",
       " '136321': '1.2.276.0.28.3.345049594267.42.11112.20220902085047014.txt',\n",
       " '136418': '1.2.276.0.28.3.345049594267.42.9996.20221021115247014.txt',\n",
       " '136425': '1.2.276.0.28.3.345049594267.42.8332.20221024073325014.txt',\n",
       " '136432': '1.2.276.0.28.3.345049594267.42.6184.20221021120141000.txt',\n",
       " '136456': '1.2.276.0.28.3.345049594267.42.380.20221027100652000.txt',\n",
       " '136470': '1.2.276.0.28.3.345049594267.42.8788.20220324114742014.txt',\n",
       " '136487': '1.2.276.0.28.3.345049594267.42.6724.20221021121610014.txt',\n",
       " '136494': '1.2.276.0.28.3.345049594267.42.7236.20221021122702000.txt',\n",
       " '136505': '1.2.276.0.28.3.345049594267.42.9296.20221021123838014.txt',\n",
       " '136550': '1.2.276.0.28.3.345049594267.42.10824.20220304131137014.txt',\n",
       " '136581': '1.2.276.0.28.3.345049594267.42.404.20220705081031014.txt',\n",
       " '138310': '1.2.276.0.28.3.345049594267.42.6016.20221025093520014.txt',\n",
       " '138421': '1.2.276.0.28.3.345049594267.42.3552.20220822144027014.txt',\n",
       " '142050': '1.2.276.0.28.3.345049594267.42.7492.20220526103425000.txt',\n",
       " '144629': '1.2.276.0.28.3.345049594267.42.5692.20221027101757014.txt',\n",
       " '146007': '1.2.276.0.28.3.345049594267.42.10596.20220610092607014.txt',\n",
       " '151080': '1.2.276.0.28.3.345049594267.42.7188.20220323115912014.txt',\n",
       " '162158': '1.2.276.0.28.3.345049594267.42.8412.20220324120319014.txt',\n",
       " '163557': '1.2.276.0.28.3.345049594267.42.11196.20220705082424014.txt',\n",
       " '165171': '1.2.276.0.28.3.345049594267.42.5352.20220705091023000.txt',\n",
       " '173559': '1.2.276.0.28.3.345049594267.42.7968.20220323150458014.txt',\n",
       " '173702': '1.2.276.0.28.3.345049594267.42.10072.20221031112423000.txt',\n",
       " '175222': '1.2.276.0.28.3.345049594267.42.6824.20221027220111000.txt',\n",
       " '180705': '1.2.276.0.28.3.345049594267.42.6828.20221027221359014.txt',\n",
       " '181086': '1.2.276.0.28.3.345049594267.42.2056.20220304123829014.txt',\n",
       " '184429': '1.2.276.0.28.3.345049594267.42.2220.20220516125713014.txt',\n",
       " '187275': '1.2.276.0.28.3.345049594267.42.12980.20220822144758014.txt',\n",
       " '197239': '1.2.276.0.28.3.345049594267.42.2404.20220705083622014.txt',\n",
       " '199391': '1.2.276.0.28.3.345049594267.42.8228.20220822154821014.txt',\n",
       " '200637': '1.2.276.0.28.3.345049594267.42.4788.20220304133018000.txt',\n",
       " '202299': '1.2.276.0.28.3.345049594267.42.6692.20220323134941014.txt',\n",
       " '203636': '1.2.276.0.28.3.345049594267.42.784.20220705091924014.txt',\n",
       " '209349': '1.2.276.0.28.3.345049594267.42.4912.20221027222334000.txt',\n",
       " '212333': '1.2.276.0.28.3.345049594267.42.3332.20220525133559000.txt',\n",
       " '215387': '1.2.276.0.28.3.345049594267.42.2788.20220705084521014.txt',\n",
       " '224879': '1.2.276.0.28.3.345049594267.42.4964.20220323164031014.txt',\n",
       " '225858': '1.2.276.0.28.3.345049594267.42.9976.20220705092638014.txt',\n",
       " '225969': '1.2.276.0.28.3.345049594267.42.6648.20220715100831014.txt',\n",
       " '226160': '1.2.276.0.28.3.345049594267.42.12456.20220513082155014.txt',\n",
       " '233427': '1.2.276.0.28.3.345049594267.42.7832.20221027223344000.txt',\n",
       " '236384': '1.2.276.0.28.3.345049594267.42.2664.20221027224747014.txt',\n",
       " '240819': '1.2.276.0.28.3.345049594267.42.1356.20220304134337014.txt',\n",
       " '246282': '1.2.276.0.28.3.345049594267.42.2704.20220705094230014.txt',\n",
       " '248597': '1.2.276.0.28.3.345049594267.42.8356.20220610093252014.txt',\n",
       " '255903': '1.2.276.0.28.3.345049594267.42.3724.20220705085329014.txt',\n",
       " '256437': '1.2.276.0.28.3.345049594267.42.4180.20220323175859014.txt',\n",
       " '259523': '1.2.276.0.28.3.345049594267.42.8616.20220829082920014.txt',\n",
       " '260788': '1.2.276.0.28.3.345049594267.42.3792.20221027225554000.txt',\n",
       " '260875': '1.2.276.0.28.3.345049594267.42.6000.20220323201108000.txt',\n",
       " '267616': '1.2.276.0.28.3.345049594267.42.5576.20220323190446014.txt',\n",
       " '268602': '1.2.276.0.28.3.345049594267.42.2056.20220324123140014.txt',\n",
       " '278319': '1.2.276.0.28.3.345049594267.42.924.20220705095007000.txt',\n",
       " '278937': '1.2.276.0.28.3.345049594267.42.7932.20220323143850014.txt',\n",
       " '282528': '1.2.276.0.28.3.345049594267.42.10140.20220304140011014.txt',\n",
       " '283229': '1.2.276.0.28.3.345049594267.42.5064.20220523073347014.txt',\n",
       " '292471': '1.2.276.0.28.3.345049594267.42.4672.20220323152404000.txt',\n",
       " '294019': '1.2.276.0.28.3.345049594267.42.8304.20220304141626000.txt',\n",
       " '295789': '1.2.276.0.28.3.345049594267.42.11064.20220525134301014.txt',\n",
       " '317884': '1.2.276.0.28.3.345049594267.42.3408.20220822151517000.txt',\n",
       " '320656': '1.2.276.0.28.3.345049594267.42.8452.20220705095935014.txt',\n",
       " '322270': '1.2.276.0.28.3.345049594267.42.5776.20221027230638014.txt',\n",
       " '326567': '1.2.276.0.28.3.345049594267.42.6668.20220516113343014.txt',\n",
       " '328327': '1.2.276.0.28.3.345049594267.42.10280.20220822153245014.txt',\n",
       " '329702': '1.2.276.0.28.3.345049594267.42.6316.20221027231634014.txt',\n",
       " '331182': '1.2.276.0.28.3.345049594267.42.5908.20220304142324014.txt',\n",
       " '332758': '1.2.276.0.28.3.345049594267.42.9824.20220523073854014.txt',\n",
       " '335382': '1.2.276.0.28.3.345049594267.42.5484.20220324124922014.txt',\n",
       " '341417': '1.2.276.0.28.3.345049594267.42.5468.20220523063431000.txt',\n",
       " '345599': '1.2.276.0.28.3.345049594267.42.10716.20220324131122000.txt',\n",
       " '345617': '1.2.276.0.28.3.345049594267.42.4868.20221027232623000.txt',\n",
       " '349904': '1.2.276.0.28.3.345049594267.42.164.20221027233746014.txt',\n",
       " '353491': '1.2.276.0.28.3.345049594267.42.10216.20220324133214000.txt',\n",
       " '357559': '1.2.276.0.28.3.345049594267.42.2460.20220323113642000.txt',\n",
       " '362198': '1.2.276.0.28.3.345049594267.42.3664.20220513100638014.txt',\n",
       " '365388': '1.2.276.0.28.3.345049594267.42.1340.20220526121642000.txt',\n",
       " '366169': '1.2.276.0.28.3.345049594267.42.8600.20220523064041014.txt',\n",
       " '369762': '1.2.276.0.28.3.345049594267.42.6744.20220323143141000.txt',\n",
       " '370347': '1.2.276.0.28.3.345049594267.42.3040.20220324135128014.txt',\n",
       " '370941': '1.2.276.0.28.3.345049594267.42.5808.20220304144523014.txt',\n",
       " '377299': '1.2.276.0.28.3.345049594267.42.844.20220323153723014.txt',\n",
       " '381804': '1.2.276.0.28.3.345049594267.42.6564.20220323111922014.txt',\n",
       " '382098': '1.2.276.0.28.3.345049594267.42.5628.20220324140127014.txt',\n",
       " '383275': '1.2.276.0.28.3.345049594267.42.5396.20220803131843014.txt',\n",
       " '383862': '1.2.276.0.28.3.345049594267.42.10024.20221027234717000.txt',\n",
       " '384136': '1.2.276.0.28.3.345049594267.42.4364.20220324142122014.txt',\n",
       " '388787': '1.2.276.0.28.3.345049594267.42.3180.20220610094201014.txt',\n",
       " '395464': '1.2.276.0.28.3.345049594267.42.10372.20220324143055014.txt',\n",
       " '396349': '1.2.276.0.28.3.345049594267.42.9756.20221027235843014.txt',\n",
       " '398338': '1.2.276.0.28.3.345049594267.42.504.20220516150756014.txt',\n",
       " '406637': '1.2.276.0.28.3.345049594267.42.2540.20220323160337014.txt',\n",
       " '406668': '1.2.276.0.28.3.345049594267.42.10168.20220324144108014.txt',\n",
       " '409945': '1.2.276.0.28.3.345049594267.42.5276.20220323181926014.txt',\n",
       " '410655': '1.2.276.0.28.3.345049594267.42.5248.20220324144854014.txt',\n",
       " '413890': '1.2.276.0.28.3.345049594267.42.2324.20220325090344000.txt',\n",
       " '414997': '1.2.276.0.28.3.345049594267.42.5168.20221028001136000.txt',\n",
       " '415018': '1.2.276.0.28.3.345049594267.42.2720.20220803133253000.txt',\n",
       " '422011': '1.2.276.0.28.3.345049594267.42.5252.20221028001927014.txt',\n",
       " '423038': '1.2.276.0.28.3.345049594267.42.6600.20220324145612014.txt',\n",
       " '425409': '1.2.276.0.28.3.345049594267.42.4480.20220715093524000.txt',\n",
       " '426933': '1.2.276.0.28.3.345049594267.42.7368.20221028003258000.txt',\n",
       " '427158': '1.2.276.0.28.3.345049594267.42.10352.20220523064921000.txt',\n",
       " '427498': '1.2.276.0.28.3.345049594267.42.10000.20220324150941000.txt',\n",
       " '428859': '1.2.276.0.28.3.345049594267.42.9556.20220610124618014.txt',\n",
       " '429703': '1.2.276.0.28.3.345049594267.42.6864.20220822155559014.txt',\n",
       " '429789': '1.2.276.0.28.3.345049594267.42.4720.20220324152424000.txt',\n",
       " '430055': '1.2.276.0.28.3.345049594267.42.8304.20220323133742014.txt',\n",
       " '435703': '1.2.276.0.28.3.345049594267.42.5856.20220523065611014.txt',\n",
       " '436591': '1.2.276.0.28.3.345049594267.42.10236.20221028004545000.txt',\n",
       " '438820': '1.2.276.0.28.3.345049594267.42.9144.20220304145417000.txt',\n",
       " '440453': '1.2.276.0.28.3.345049594267.42.8600.20220523070610014.txt',\n",
       " '449790': '1.2.276.0.28.3.345049594267.42.4736.20220610125410000.txt',\n",
       " '451989': '1.2.276.0.28.3.345049594267.42.2488.20220324160204014.txt',\n",
       " '452500': '1.2.276.0.28.3.345049594267.42.5676.20220324160745014.txt',\n",
       " '458362': '1.2.276.0.28.3.345049594267.42.7436.20220324161516014.txt',\n",
       " '458522': '1.2.276.0.28.3.345049594267.42.7764.20220527130000014.txt',\n",
       " '475503': '1.2.276.0.28.3.345049594267.42.5840.20220610091600014.txt',\n",
       " '480982': '1.2.276.0.28.3.345049594267.42.6716.20220519095627014.txt',\n",
       " '482082': '1.2.276.0.28.3.345049594267.42.6392.20220323193946014.txt',\n",
       " '485925': '1.2.276.0.28.3.345049594267.42.6828.20220613103337000.txt',\n",
       " '490144': '1.2.276.0.28.3.345049594267.42.10596.20220705101243000.txt',\n",
       " '493907': '1.2.276.0.28.3.345049594267.42.1188.20220324162524014.txt',\n",
       " '499832': '1.2.276.0.28.3.345049594267.42.11108.20220715101836014.txt',\n",
       " '503559': '1.2.276.0.28.3.345049594267.42.7372.20220324163348000.txt',\n",
       " '503788': '1.2.276.0.28.3.345049594267.42.6280.20220304145925014.txt',\n",
       " '507704': '1.2.276.0.28.3.345049594267.42.1836.20220304150448000.txt',\n",
       " '512145': '1.2.276.0.28.3.345049594267.42.8184.20220324200905014.txt',\n",
       " '518709': '1.2.276.0.28.3.345049594267.42.7324.20220705102405014.txt',\n",
       " '521779': '1.2.276.0.28.3.345049594267.42.6960.20220715094327014.txt',\n",
       " '522692': '1.2.276.0.28.3.345049594267.42.4652.20220822145710014.txt',\n",
       " '523688': '1.2.276.0.28.3.345049594267.42.11248.20220525132210014.txt',\n",
       " '537519': '1.2.276.0.28.3.345049594267.42.2468.20220523071405014.txt',\n",
       " '540527': '1.2.276.0.28.3.345049594267.42.1104.20220323200106000.txt',\n",
       " '545508': '1.2.276.0.28.3.345049594267.42.1288.20220829083504014.txt',\n",
       " '547028': '1.2.276.0.28.3.345049594267.42.5240.20220304103421014.txt',\n",
       " '552612': '1.2.276.0.28.3.345049594267.42.800.20220304151258000.txt',\n",
       " '570078': '1.2.276.0.28.3.345049594267.42.3240.20221028010813000.txt',\n",
       " '570103': '1.2.276.0.28.3.345049594267.42.4076.20220324164819014.txt',\n",
       " '575597': '1.2.276.0.28.3.345049594267.42.8348.20221028012127000.txt',\n",
       " '576496': '1.2.276.0.28.3.345049594267.42.7700.20221028013331014.txt',\n",
       " '582854': '1.2.276.0.28.3.345049594267.42.8688.20220705104201014.txt',\n",
       " '585377': '1.2.276.0.28.3.345049594267.42.2432.20220610131202000.txt',\n",
       " '591162': '1.2.276.0.28.3.345049594267.42.5880.20220324165826014.txt',\n",
       " '592863': '1.2.276.0.28.3.345049594267.42.4192.20220304152003014.txt',\n",
       " '608593': '1.2.276.0.28.3.345049594267.42.7368.20221028014451014.txt',\n",
       " '612257': '1.2.276.0.28.3.345049594267.42.3240.20221031113458000.txt',\n",
       " '613569': '1.2.276.0.28.3.345049594267.42.10776.20220526105023014.txt',\n",
       " '617769': '1.2.276.0.28.3.345049594267.42.5132.20220304153726014.txt',\n",
       " '621638': '1.2.276.0.28.3.345049594267.42.764.20220323140730014.txt',\n",
       " '632817': '1.2.276.0.28.3.345049594267.42.11196.20220614104341000.txt',\n",
       " '633549': '1.2.276.0.28.3.345049594267.42.3300.20220304155001000.txt',\n",
       " '639745': '1.2.276.0.28.3.345049594267.42.10956.20220324172400000.txt',\n",
       " '640431': '1.2.276.0.28.3.345049594267.42.10752.20220523074753014.txt',\n",
       " '648324': '1.2.276.0.28.3.345049594267.42.2000.20220803134034014.txt',\n",
       " '648879': '1.2.276.0.28.3.345049594267.42.9308.20220513114031014.txt',\n",
       " '652446': '1.2.276.0.28.3.345049594267.42.11248.20220912075056014.txt',\n",
       " '654157': '1.2.276.0.28.3.345049594267.42.7740.20221028015613014.txt',\n",
       " '654560': '1.2.276.0.28.3.345049594267.42.8336.20220323183110000.txt',\n",
       " '660928': '1.2.276.0.28.3.345049594267.42.3804.20220304155829000.txt',\n",
       " '661896': '1.2.276.0.28.3.345049594267.42.8068.20221028020641000.txt',\n",
       " '662368': '1.2.276.0.28.3.345049594267.42.9408.20220324174301014.txt',\n",
       " '663854': '1.2.276.0.28.3.345049594267.42.10040.20220705104954014.txt',\n",
       " '666919': '1.2.276.0.28.3.345049594267.42.4632.20221028021850000.txt',\n",
       " '670208': '1.2.276.0.28.3.345049594267.42.856.20220304161042014.txt',\n",
       " '673634': '1.2.276.0.28.3.345049594267.42.2616.20220613104228014.txt',\n",
       " '682119': '1.2.276.0.28.3.345049594267.42.7308.20220526110121014.txt',\n",
       " '685823': '1.2.276.0.28.3.345049594267.42.7144.20220323151425000.txt',\n",
       " '693232': '1.2.276.0.28.3.345049594267.42.6812.20220323145021014.txt',\n",
       " '694433': '1.2.276.0.28.3.345049594267.42.3152.20220527114438014.txt',\n",
       " '695790': '1.2.276.0.28.3.345049594267.42.8428.20221018093528014.txt',\n",
       " '697807': '1.2.276.0.28.3.345049594267.42.9932.20221018094559014.txt',\n",
       " '698737': '1.2.276.0.28.3.345049594267.42.4468.20221018095635014.txt',\n",
       " '699976': '1.2.276.0.28.3.345049594267.42.9828.20220323141620014.txt',\n",
       " '700458': '1.2.276.0.28.3.345049594267.42.11216.20220526104500014.txt',\n",
       " '701739': '1.2.276.0.28.3.345049594267.42.7532.20221018100518014.txt',\n",
       " '703242': '1.2.276.0.28.3.345049594267.42.3456.20221018101720014.txt',\n",
       " '704981': '1.2.276.0.28.3.345049594267.42.2808.20221018102601000.txt',\n",
       " '705786': '1.2.276.0.28.3.345049594267.42.8820.20221018103504014.txt',\n",
       " '706029': '1.2.276.0.28.3.345049594267.42.10576.20220324175217000.txt',\n",
       " '711977': '1.2.276.0.28.3.345049594267.42.5016.20220822141333014.txt',\n",
       " '712807': '1.2.276.0.28.3.345049594267.42.6524.20220323125421014.txt',\n",
       " '713633': '1.2.276.0.28.3.345049594267.42.1060.20221018104355014.txt',\n",
       " '720143': '1.2.276.0.28.3.345049594267.42.5880.20220323184001014.txt',\n",
       " '720754': '1.2.276.0.28.3.345049594267.42.8320.20220822161317014.txt',\n",
       " '738038': '1.2.276.0.28.3.345049594267.42.5032.20220526113838014.txt',\n",
       " '742438': '1.2.276.0.28.3.345049594267.42.5828.20220323175146000.txt',\n",
       " '745833': '1.2.276.0.28.3.345049594267.42.2932.20220526111709014.txt',\n",
       " '746978': '1.2.276.0.28.3.345049594267.42.4188.20220323184858014.txt',\n",
       " '749366': '1.2.276.0.28.3.345049594267.42.8052.20220323192734014.txt',\n",
       " '751055': '1.2.276.0.28.3.345049594267.42.4192.20221018105326014.txt',\n",
       " '753648': '1.2.276.0.28.3.345049594267.42.8084.20221018110340014.txt',\n",
       " '754238': '1.2.276.0.28.3.345049594267.42.5308.20220304163536014.txt',\n",
       " '757591': '1.2.276.0.28.3.345049594267.42.8600.20220304164851014.txt',\n",
       " '760443': '1.2.276.0.28.3.345049594267.42.10220.20221018111242014.txt',\n",
       " '761262': '1.2.276.0.28.3.345049594267.42.7448.20221028023806000.txt',\n",
       " '765028': '1.2.276.0.28.3.345049594267.42.10236.20221018112206000.txt',\n",
       " '770087': '1.2.276.0.28.3.345049594267.42.9740.20220519100423014.txt',\n",
       " '770778': '1.2.276.0.28.3.345049594267.42.9396.20220304101731014.txt',\n",
       " '771757': '1.2.276.0.28.3.345049594267.42.2164.20221018113138014.txt',\n",
       " '773499': '1.2.276.0.28.3.345049594267.42.8936.20220519101401014.txt',\n",
       " '773753': '1.2.276.0.28.3.345049594267.42.3308.20220323174127014.txt',\n",
       " '777366': '1.2.276.0.28.3.345049594267.42.3716.20221018115751014.txt',\n",
       " '779737': '1.2.276.0.28.3.345049594267.42.3592.20220323170520014.txt',\n",
       " '781339': '1.2.276.0.28.3.345049594267.42.3832.20221028024709014.txt',\n",
       " '786209': '1.2.276.0.28.3.345049594267.42.9652.20221018120735014.txt',\n",
       " '786438': '1.2.276.0.28.3.345049594267.42.5548.20221018121601014.txt',\n",
       " '788021': '1.2.276.0.28.3.345049594267.42.6680.20220323162946014.txt',\n",
       " '788507': '1.2.276.0.28.3.345049594267.42.4844.20220526113340014.txt',\n",
       " '789586': '1.2.276.0.28.3.345049594267.42.3700.20220324180122014.txt',\n",
       " '790703': '1.2.276.0.28.3.345049594267.42.1108.20221018130917000.txt',\n",
       " '792990': '1.2.276.0.28.3.345049594267.42.208.20220323122025014.txt',\n",
       " '794229': '1.2.276.0.28.3.345049594267.42.6720.20221018131829014.txt',\n",
       " '794378': '1.2.276.0.28.3.345049594267.42.4244.20221018132815014.txt',\n",
       " '794632': '1.2.276.0.28.3.345049594267.42.8084.20221018133748014.txt',\n",
       " '801136': '1.2.276.0.28.3.345049594267.42.10964.20220513123824014.txt',\n",
       " '801358': '1.2.276.0.28.3.345049594267.42.10544.20220304125132014.txt',\n",
       " '801365': '1.2.276.0.28.3.345049594267.42.10024.20221028025551000.txt',\n",
       " '808262': '1.2.276.0.28.3.345049594267.42.156.20220523072034014.txt',\n",
       " '810826': '1.2.276.0.28.3.345049594267.42.4804.20220523075645014.txt',\n",
       " '811041': '1.2.276.0.28.3.345049594267.42.8588.20220304165445000.txt',\n",
       " '817358': '1.2.276.0.28.3.345049594267.42.10160.20220614105230014.txt',\n",
       " '822195': '1.2.276.0.28.3.345049594267.42.8140.20221018134708014.txt',\n",
       " '822640': '1.2.276.0.28.3.345049594267.42.8724.20221018140111014.txt',\n",
       " '840797': '1.2.276.0.28.3.345049594267.42.9656.20220526114341014.txt',\n",
       " '845334': '1.2.276.0.28.3.345049594267.42.9280.20220831095613014.txt',\n",
       " '845594': '1.2.276.0.28.3.345049594267.42.10412.20220829091629014.txt',\n",
       " '857769': '1.2.276.0.28.3.345049594267.42.5696.20221018141042014.txt',\n",
       " '860079': '1.2.276.0.28.3.345049594267.42.7360.20220304171453000.txt',\n",
       " '860104': '1.2.276.0.28.3.345049594267.42.5388.20220527125203000.txt',\n",
       " '860791': '1.2.276.0.28.3.345049594267.42.4036.20220526115557000.txt',\n",
       " '864279': '1.2.276.0.28.3.345049594267.42.6604.20221018141932014.txt',\n",
       " '866157': '1.2.276.0.28.3.345049594267.42.7112.20221018142741014.txt',\n",
       " '866164': '1.2.276.0.28.3.345049594267.42.7120.20220304172108014.txt',\n",
       " '866522': '1.2.276.0.28.3.345049594267.42.9660.20220829082051014.txt',\n",
       " '868837': '1.2.276.0.28.3.345049594267.42.6112.20221018143701014.txt',\n",
       " '868924': '1.2.276.0.28.3.345049594267.42.10236.20220526120706014.txt',\n",
       " '869330': '1.2.276.0.28.3.345049594267.42.2828.20221018144531000.txt',\n",
       " '870199': '1.2.276.0.28.3.345049594267.42.7220.20220705105655000.txt',\n",
       " '873698': '1.2.276.0.28.3.345049594267.42.4092.20220523080312014.txt',\n",
       " '873872': '1.2.276.0.28.3.345049594267.42.9704.20221018145343000.txt',\n",
       " '876420': '1.2.276.0.28.3.345049594267.42.9940.20221018150348014.txt',\n",
       " '881101': '1.2.276.0.28.3.345049594267.42.6148.20221018151058014.txt',\n",
       " '884513': '1.2.276.0.28.3.345049594267.42.11164.20220829075917014.txt',\n",
       " '886175': '1.2.276.0.28.3.345049594267.42.9816.20221018151947014.txt',\n",
       " '887918': '1.2.276.0.28.3.345049594267.42.9900.20221018152848014.txt',\n",
       " '888119': '1.2.276.0.28.3.345049594267.42.5484.20221028030551014.txt',\n",
       " '889105': '1.2.276.0.28.3.345049594267.42.1360.20221018153944014.txt',\n",
       " '891238': '1.2.276.0.28.3.345049594267.42.4992.20220324182549014.txt',\n",
       " '892519': '1.2.276.0.28.3.345049594267.42.1300.20220324184534009.txt',\n",
       " '900450': '1.2.276.0.28.3.345049594267.42.10028.20221018154844014.txt',\n",
       " '910698': '1.2.276.0.28.3.345049594267.42.7160.20220715095259014.txt',\n",
       " '914836': '1.2.276.0.28.3.345049594267.42.10764.20220323192041000.txt',\n",
       " '916856': '1.2.276.0.28.3.345049594267.42.9876.20221028031938000.txt',\n",
       " '920645': '1.2.276.0.28.3.345049594267.42.5688.20221028033720014.txt',\n",
       " '927080': '1.2.276.0.28.3.345049594267.42.1236.20220527130812014.txt',\n",
       " '927431': '1.2.276.0.28.3.345049594267.42.10808.20220526104009014.txt',\n",
       " '935406': '1.2.276.0.28.3.345049594267.42.4816.20220527122236000.txt',\n",
       " '940309': '1.2.276.0.28.3.345049594267.42.11248.20220323173321014.txt',\n",
       " '943797': '1.2.276.0.28.3.345049594267.42.1060.20221028034905014.txt',\n",
       " '944714': '1.2.276.0.28.3.345049594267.42.11192.20220523081138014.txt',\n",
       " '951248': '1.2.276.0.28.3.345049594267.42.4156.20220829090031000.txt',\n",
       " '968869': '1.2.276.0.28.3.345049594267.42.10592.20220527124040014.txt',\n",
       " '971099': '1.2.276.0.28.3.345049594267.42.5544.20220304175200014.txt',\n",
       " '979159': '1.2.276.0.28.3.345049594267.42.420.20220526105646000.txt',\n",
       " '985215': '1.2.276.0.28.3.345049594267.42.1384.20220304181132000.txt',\n",
       " '985437': '1.2.276.0.28.3.345049594267.42.7580.20220829080800014.txt',\n",
       " '986374': '1.2.276.0.28.3.345049594267.42.2108.20220705111053014.txt',\n",
       " '988394': '1.2.276.0.28.3.345049594267.42.2464.20220715095932014.txt',\n",
       " '988752': '1.2.276.0.28.3.345049594267.42.11240.20220527122820014.txt',\n",
       " '991277': '1.2.276.0.28.3.345049594267.42.8472.20220304183622014.txt',\n",
       " '993242': '1.2.276.0.28.3.345049594267.42.5620.20220323161644014.txt',\n",
       " '997831': '1.2.276.0.28.3.345049594267.42.2584.20221028035850000.txt',\n",
       " '998310': '1.2.276.0.28.3.345049594267.42.5580.20220324202039000.txt'}"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nodule_dict_final_date"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "379f9d5f",
   "metadata": {},
   "source": [
    "### Save dictionaries as pickle to be used to find actual files with information as provided by AI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "ef5c7634",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Save those files in dictionary to be used by the main file to add to REDCap\n",
    "with open('patient_and_files_nodules.pkl', 'wb') as f:\n",
    "    pickle.dump(nodule_dict_final_date, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "a5e74a23",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('patient_and_files_aorta.pkl', 'wb') as f:\n",
    "    pickle.dump(aorta_dict_final_date, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "c8fe3c4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('patient_and_files_CACS.pkl', 'wb') as f:\n",
    "    pickle.dump(CACS_dict_final_date, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "9b526148",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('patient_and_files_vertebra.pkl', 'wb') as f:\n",
    "    pickle.dump(vertebra_dict_final_date, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "a83a0bc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('patient_and_files_fat.pkl', 'wb') as f:\n",
    "    pickle.dump(cardiac_fat_dict_final_date, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "1faeb105",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('patient_and_files_emph.pkl', 'wb') as f:\n",
    "    pickle.dump(emph_dict_final_date, f)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "357e7034",
   "metadata": {},
   "source": [
    "### Check that there are unique files for nodules, aorta, and CACS measurements (for the rest assumed we won't have any issues)\n",
    "\n",
    "### Confirm that not the same file in more than one groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "7978d10c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Total number of common files between aorta and nodules is:  0\n",
      "Total number of common files between CACS and nodules is:  0\n"
     ]
    }
   ],
   "source": [
    "#Check if participants in CACS list exist in aorta list\n",
    "for CACS_pat in list(CACS_dict_final.keys()):\n",
    "    if CACS_pat in list(aorta_dict_final.keys()):\n",
    "        if aorta_dict_final[CACS_pat]==CACS_dict_final[CACS_pat]:\n",
    "            print(\"Participant {} exists in both aorta and CACS list with the same file name which is {}\".\n",
    "                  format(CACS_pat, CACS_dict_final[CACS_pat]))\n",
    "\n",
    "#Check if participants in aorta list exist in CACS list\n",
    "for aorta_pat in list(aorta_dict_final.keys()):\n",
    "    if aorta_pat in list(CACS_dict_final.keys()):\n",
    "        if aorta_dict_final[aorta_pat]==CACS_dict_final[aorta_pat]:\n",
    "            print(\"Participant {} exists in both aorta and CACS list with the same file name which is {}\".\n",
    "                  format(aorta_pat, CACS_dict_final[aorta_pat]))\n",
    "            \n",
    "#For the above two lists, we don't expect any common files - otherwise error!\n",
    "\n",
    "\n",
    "#Initialize two counters to keep track number of common files between nodules list and aorta and CACS lists\n",
    "aorta_and_nodules=0\n",
    "CACS_and_nodules=0\n",
    "\n",
    "for nodule_pat in list(nodule_dict_final_date.keys()):#nodule_dict_final.keys()):\n",
    "\n",
    "    #Check if participants in nodule list exist in aorta list\n",
    "    if nodule_pat in list(aorta_dict_final.keys()):\n",
    "        if aorta_dict_final[nodule_pat]==nodule_dict_final[nodule_pat]:\n",
    "            print(\"Participant {} exists in both aorta and nodules list with the same file name which is {}\".\n",
    "                  format(nodule_pat, aorta_dict_final[nodule_pat]))\n",
    "            aorta_and_nodules=aorta_and_nodules+1\n",
    "    \n",
    "    #Check if participants in nodule list exist in CACS list\n",
    "    if nodule_pat in list(CACS_dict_final.keys()):\n",
    "        if CACS_dict_final[nodule_pat]==nodule_dict_final[nodule_pat]:\n",
    "            print(\"Participant {} exists in both CACS and nodules list with the same file name which is {}\".\n",
    "                  format(nodule_pat, CACS_dict_final[nodule_pat]))\n",
    "            CACS_and_nodules=CACS_and_nodules+1\n",
    "            \n",
    "    #Check if participants in nodule list exist in both aorta and CACS lists\n",
    "    if nodule_pat in list(CACS_dict_final.keys()) and nodule_pat in list(aorta_dict_final.keys()):\n",
    "        if CACS_dict_final[nodule_pat]==nodule_dict_final[nodule_pat] and aorta_dict_final[nodule_pat]==nodule_dict_final[nodule_pat]:\n",
    "            print(\"Participant {} exists in ALL nodule, aorta, and CACS lists!\".format(nodule_pat))\n",
    "\n",
    "print('\\n')\n",
    "print(\"Total number of common files between aorta and nodules is: \",aorta_and_nodules)\n",
    "print(\"Total number of common files between CACS and nodules is: \",CACS_and_nodules)\n",
    "#We should not have any common files since different kernels sent for each of them"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a100ce7",
   "metadata": {},
   "source": [
    "### Compare files between two different folders - Not needed anymore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "ac346d28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Takes ~33min to run for 9616 files in a i7-10750H 2.6GHz CPU\n",
    "\n",
    "# all_files=os.listdir(path) #All txt files\n",
    "# files_left=all_files.copy() #A copy of them to gradually remove files already looped - check below\n",
    "\n",
    "# patient_and_files={} #Empty dictionary to be filled with the patient_id and all txt files that correspond to it\n",
    "\n",
    "# for file in tqdm(all_files):\n",
    "#     pat_id,_=get_patient_id(path,file) #Get patient_id in a txt file\n",
    "    \n",
    "#     if pat_id not in list(patient_and_files.keys()) and pat_id!='': #If it's a valid patient and not already added in dict\n",
    "        \n",
    "#         patient_and_files[pat_id]=[file] #Add patient and txt file to dictionary\n",
    "\n",
    "#         to_be_deleted=[file] #Add that file in the list of files to be deleted\n",
    "#         for file_check in files_left: #Check remaining files to see if this participant exists more than once\n",
    "#             pat_id_check,_=get_patient_id(path,file_check) #Get patient_id for each of the remaining txt files\n",
    "\n",
    "#             if pat_id==pat_id_check and file!=file_check: #If we have the same patient and not the same file name\n",
    "#                 patient_and_files[pat_id].append(file_check) #Add txt file to dictionary list\n",
    "#                 to_be_deleted.append(file_check) #Add that file in the list of files to be deleted\n",
    "\n",
    "#         if file in files_left: #If the file has not be deleted from the list of remaining files\n",
    "#             for file_del in to_be_deleted:\n",
    "#                 files_left.remove(file_del) #Delete it, along with all the rest in which the same patient appears\n",
    "            \n",
    "# patient_and_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "cbafdb10",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Save those files in dictionary\n",
    "# with open('patient_and_files.pkl', 'wb') as f:\n",
    "#     pickle.dump(patient_and_files, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1743320e",
   "metadata": {},
   "source": [
    "### Load dictionary with participant and all the txt files that correspond to it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "d00d7456",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Retrieve files as dictionary\n",
    "# with open('patient_and_files.pkl', 'rb') as f:\n",
    "#     patient_and_files = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "946233e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(dict(sorted(patient_and_files.items())))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d64a3202",
   "metadata": {},
   "source": [
    "##### Problems with 714969 and 109077 fixed now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "7ae48cd9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total time to run was 20.0094952583313secs\n"
     ]
    }
   ],
   "source": [
    "end=time.time()\n",
    "print('Total time to run was {}secs'.format(end-start)) #~20secs to run in a i7-10750H 2.6GHz CPU for 10k files"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
