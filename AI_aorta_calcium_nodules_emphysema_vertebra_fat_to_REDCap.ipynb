{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0da3fece",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import dependencies\n",
    "import os\n",
    "import numpy as np\n",
    "import traceback\n",
    "import pandas as pd\n",
    "import time\n",
    "import pickle\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore',category=FutureWarning) #Avoid printing warning messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4620d467",
   "metadata": {},
   "outputs": [],
   "source": [
    "start=time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5e114fc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "AI_path=os.getcwd()+\"\\\\ai_logs\" #Path of folder with AI txt files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b7756b59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stored 'path' (str)\n"
     ]
    }
   ],
   "source": [
    "path=os.getcwd()+'\\AI_timestamp' #Path of all AI txt files with processing time and tasks\n",
    "\n",
    "#Store this variable - Will be accessed inside the other notebook \n",
    "%store path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dde70ddd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10029/10029 [00:02<00:00, 4408.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of files without any of the above measurements (nodules, aorta, CACS, vertebra, cardiac fat) is: 0\n",
      "Total number of empty files is: 492\n",
      "Total number of files with measurements is: 9528\n",
      "Total number of files with empty patient_names: 9\n",
      "Unique participants sent for nodules:  375\n",
      "All participants sent for nodules:  547\n",
      "\n",
      "\n",
      "Unique participants sent for aorta:  1580\n",
      "All participants sent for aorta:  1685\n",
      "\n",
      "\n",
      "Unique participants sent for CACS:  4965\n",
      "All participants sent for CACS:  6951\n",
      "\n",
      "\n",
      "Unique participants sent for vertebra:  1880\n",
      "All participants sent for vertebra:  2081\n",
      "\n",
      "\n",
      "Unique participants sent for cardiac fat:  1647\n",
      "All participants sent for cardiac fat:  1648\n",
      "\n",
      "\n",
      "Unique participants sent for emphysema:  1883\n",
      "All participants sent for emphysema:  2116\n",
      "Total number of files with aorta measurements 1580\n",
      "Total number of files with CACS measurements 4965\n",
      "Total number of files with nodule measurements 375\n",
      "Total number of files with vertebra measurements 1880\n",
      "Total number of files with cardiac fat measurements 1647\n",
      "Total number of files with emphysema measurements 1883\n",
      "Total number of files with CACS measurements after 1st May 2022 3651\n",
      "Total number of files with aorta measurements after 1st September 2022 1368\n",
      "Total number of files with nodule measurements after 1st February 2022 334\n",
      "Total number of files with vertebra measurements after 1st September 2022 1654\n",
      "Total number of files with CACS measurements after 1st September 2022 1647\n",
      "Total number of files with emphysema measurements after 1st September 2022 1654\n",
      "\n",
      "\n",
      "Total number of common files between aorta and nodules is:  0\n",
      "Total number of common files between CACS and nodules is:  0\n",
      "Total time to run was 16.92719602584839secs\n"
     ]
    }
   ],
   "source": [
    "#Run notebook 'AI_timestamp.ipynb' to find which txt filename corresponds to which participant.\n",
    "#Continue execution on next cell if it gives error\n",
    "\n",
    "try: #To ignore error and continue in next cell we need try-except and 'no raise error' flag\n",
    "    %run ./AI_timestamp.ipynb --no-raise-error\n",
    "except:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2d5ea343",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load paths of pickle files containing dictionaries with participant and its corresponding txt file to keep - Created from 'AI_timestamp.ipynb'\n",
    "nodule_path='patient_and_files_nodules.pkl'\n",
    "aorta_path='patient_and_files_aorta.pkl'\n",
    "CACS_path='patient_and_files_CACS.pkl'\n",
    "vertebra_path='patient_and_files_vertebra.pkl'\n",
    "fat_path='patient_and_files_fat.pkl'\n",
    "emph_path='patient_and_files_emph.pkl'"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "225fc2ef",
   "metadata": {},
   "source": [
    "### REDCap attributes to be extracted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4433ab19",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Participant ID\n",
    "participant_id=['participant_id']\n",
    "\n",
    "#Aorta measurements\n",
    "diam_names=['sin_vals_diam_ai','sino_junc_diam_ai','mid_asc_aorta_diam_ai','prox_arch_diam_ai','mid_arch_diam_ai',\n",
    "            'prox_desc_diam_ai','mid_desc_diam_ai','diaphragm_diam_ai','abd_aorta_diam_ai']\n",
    "orth_names=['sin_vals_ortho_diam_ai','sino_junc_ortho_diam_ai','mid_asc_aorta_ortho_diam_ai','prox_arch_ortho_diam_ai',\n",
    "            'mid_arch_ortho_diam_ai','prox_desc_ortho_diam_ai','mid_desc_ortho_diam_ai','diaphragm_ortho_diam_ai',\n",
    "            'abd_aorta_ortho_diam_ai']\n",
    "\n",
    "#Emphysema\n",
    "emphysema=['lung_vol_ai','lung_dens_ai_wl','lung_perc15_ai_wl',\n",
    "           'lung_dens_ai_lul','lung_perc15_ai_lul',\n",
    "           'lung_dens_ai_lll','lung_perc15_ai_lll',\n",
    "           'lung_dens_ai_rul','lung_perc15_ai_rul',\n",
    "           'lung_dens_ai_ml','lung_perc15_ai_ml', \n",
    "           'lung_dens_ai_rll','lung_perc15_ai_rll',\n",
    "           'ai_emphysema_quantification_complete' #set to 1=> unverified (to check automated imports manually)\n",
    "          ]\n",
    "\n",
    "#Cardiac Fat\n",
    "heart_fat=['heart_vol_ai','cardiac_fat_vol_ai','cardiac_fat_hu','cardiac_fat_hu_sd']\n",
    "\n",
    "#Vertebra measurements\n",
    "vert_t1=['vert_volume_t1','vert_post_height_t1','vert_ant_height_t1','vert_mid_height_t1','vert_mean_hu_t1']\n",
    "vert_t2=['vert_volume_t2','vert_post_height_t2','vert_ant_height_t2','vert_mid_height_t2','vert_mean_hu_t2']\n",
    "vert_t3=['vert_volume_t3','vert_post_height_t3','vert_ant_height_t3','vert_mid_height_t3','vert_mean_hu_t3']\n",
    "vert_t4=['vert_volume_t4','vert_post_height_t4','vert_ant_height_t4','vert_mid_height_t4','vert_mean_hu_t4']\n",
    "vert_t5=['vert_volume_t5','vert_post_height_t5','vert_ant_height_t5','vert_mid_height_t5','vert_mean_hu_t5']\n",
    "vert_t6=['vert_volume_t6','vert_post_height_t6','vert_ant_height_t6','vert_mid_height_t6','vert_mean_hu_t6']\n",
    "vert_t7=['vert_volume_t7','vert_post_height_t7','vert_ant_height_t7','vert_mid_height_t7','vert_mean_hu_t7']\n",
    "vert_t8=['vert_volume_t8','vert_post_height_t8','vert_ant_height_t8','vert_mid_height_t8','vert_mean_hu_t8']\n",
    "vert_t9=['vert_volume_t9','vert_post_height_t9','vert_ant_height_t9','vert_mid_height_t9','vert_mean_hu_t9']\n",
    "vert_t10=['vert_volume_t10','vert_post_height_t10','vert_ant_height_t10','vert_mid_height_t10','vert_mean_hu_t10']\n",
    "vert_t11=['vert_volume_t11','vert_post_height_t11','vert_ant_height_t11','vert_mid_height_t11','vert_mean_hu_t11']\n",
    "vert_t12=['vert_volume_t12','vert_post_height_t12','vert_ant_height_t12','vert_mid_height_t12','vert_mean_hu_t12']\n",
    "vert_l1=['vert_volume_l1','vert_post_height_l1','vert_ant_height_l1','vert_mid_height_l1','vert_mean_hu_l1']\n",
    "vert_c7=['vert_volume_c7','vert_post_height_c7','vert_ant_height_c7','vert_mid_height_c7','vert_mean_hu_c7']\n",
    "\n",
    "#Calcium score measurements\n",
    "CACS_perf=['ai_cacs_perf']\n",
    "total_ag=['ai_total_number_lesions','ai_total_artery_agatston','ai_total_artery_volume','ai_total_artery_mass']\n",
    "total_lm=['ai_lm_number_lesions','ai_lm_artery_agatston','ai_lm_artery_volume','ai_lm_artery_mass']\n",
    "total_lad=['ai_lad_number_lesions','ai_lad_artery_agatston','ai_lad_artery_volume','ai_lad_artery_mass']\n",
    "total_cx=['ai_cx_number_lesions','ai_cx_artery_agatston','ai_cx_artery_volume','ai_cx_artery_mass']\n",
    "total_rca=['ai_rca_number_lesions','ai_rca_artery_agatston','ai_rca_artery_volume','ai_rca_artery_mass']\n",
    "\n",
    "#Lung Nodules\n",
    "nod_ids=['ai_nod_id1','ai_nod_id2','ai_nod_id3','ai_nod_id4','ai_nod_id5','ai_nod_id6','ai_nod_id7','ai_nod_id8',\n",
    "        'ai_nod_id9','ai_nod_id10']\n",
    "nod_volumes=['ai_nod_vol1','ai_nod_vol2','ai_nod_vol3','ai_nod_vol4','ai_nod_vol5','ai_nod_vol6','ai_nod_vol7',\n",
    "            'ai_nod_vol8','ai_nod_vol9','ai_nod_vol10']\n",
    "nod_diam_2d=['ai_nod_dia2d_n1','ai_nod_dia2d_n2','ai_nod_dia2d_n3','ai_nod_dia2d_n4','ai_nod_dia2d_n5',\n",
    "            'ai_nod_dia2d_n6','ai_nod_dia2d_n7','ai_nod_dia2d_n8','ai_nod_dia2d_n9','ai_nod_dia2d_n10']\n",
    "nod_diam_3d=['ai_nod_dia3d_n1','ai_nod_dia3d_n2','ai_nod_dia3d_n3','ai_nod_dia3d_n4','ai_nod_dia3d_n5',\n",
    "            'ai_nod_dia3d_n6','ai_nod_dia3d_n7','ai_nod_dia3d_n8','ai_nod_dia3d_n9','ai_nod_dia3d_n10']\n",
    "\n",
    "nod_pos=['pos1','pos2','pos3','pos4','pos5','pos6','pos7','pos8','pos9','pos10'] #Used to export file for automation\n",
    "\n",
    "#Can be used in combination with automation algorithm - Defines if a finding was TP\n",
    "# nod_true_or_not=['ai_det_true_nod1','ai_det_true_nod2','ai_det_true_nod3','ai_det_true_nod4','ai_det_true_nod5',\n",
    "#                 'ai_det_true_nod6','ai_det_true_nod7','ai_det_true_nod8','ai_det_true_nod9','ai_det_true_nod10']"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "3b094641",
   "metadata": {},
   "source": [
    "### Empty dataframes creation - They have one row with NaNs and the above columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3e747558",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>participant_id</th>\n",
       "      <th>vert_volume_t1</th>\n",
       "      <th>vert_post_height_t1</th>\n",
       "      <th>vert_ant_height_t1</th>\n",
       "      <th>vert_mid_height_t1</th>\n",
       "      <th>vert_mean_hu_t1</th>\n",
       "      <th>vert_volume_t2</th>\n",
       "      <th>vert_post_height_t2</th>\n",
       "      <th>vert_ant_height_t2</th>\n",
       "      <th>vert_mid_height_t2</th>\n",
       "      <th>...</th>\n",
       "      <th>vert_volume_l1</th>\n",
       "      <th>vert_post_height_l1</th>\n",
       "      <th>vert_ant_height_l1</th>\n",
       "      <th>vert_mid_height_l1</th>\n",
       "      <th>vert_mean_hu_l1</th>\n",
       "      <th>vert_volume_c7</th>\n",
       "      <th>vert_post_height_c7</th>\n",
       "      <th>vert_ant_height_c7</th>\n",
       "      <th>vert_mid_height_c7</th>\n",
       "      <th>vert_mean_hu_c7</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows × 71 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  participant_id vert_volume_t1 vert_post_height_t1 vert_ant_height_t1  \\\n",
       "0            NaN            NaN                 NaN                NaN   \n",
       "\n",
       "  vert_mid_height_t1 vert_mean_hu_t1 vert_volume_t2 vert_post_height_t2  \\\n",
       "0                NaN             NaN            NaN                 NaN   \n",
       "\n",
       "  vert_ant_height_t2 vert_mid_height_t2  ... vert_volume_l1  \\\n",
       "0                NaN                NaN  ...            NaN   \n",
       "\n",
       "  vert_post_height_l1 vert_ant_height_l1 vert_mid_height_l1 vert_mean_hu_l1  \\\n",
       "0                 NaN                NaN                NaN             NaN   \n",
       "\n",
       "  vert_volume_c7 vert_post_height_c7 vert_ant_height_c7 vert_mid_height_c7  \\\n",
       "0            NaN                 NaN                NaN                NaN   \n",
       "\n",
       "  vert_mean_hu_c7  \n",
       "0             NaN  \n",
       "\n",
       "[1 rows x 71 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_vertebra=pd.DataFrame(index=np.arange(1),columns=participant_id+vert_t1+vert_t2+vert_t3+vert_t4+vert_t5+vert_t6+\n",
    "                         vert_t7+vert_t8+vert_t9+vert_t10+vert_t11+vert_t12+vert_l1+vert_c7)\n",
    "df_vertebra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6ffc1046",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>participant_id</th>\n",
       "      <th>sin_vals_diam_ai</th>\n",
       "      <th>sino_junc_diam_ai</th>\n",
       "      <th>mid_asc_aorta_diam_ai</th>\n",
       "      <th>prox_arch_diam_ai</th>\n",
       "      <th>mid_arch_diam_ai</th>\n",
       "      <th>prox_desc_diam_ai</th>\n",
       "      <th>mid_desc_diam_ai</th>\n",
       "      <th>diaphragm_diam_ai</th>\n",
       "      <th>abd_aorta_diam_ai</th>\n",
       "      <th>sin_vals_ortho_diam_ai</th>\n",
       "      <th>sino_junc_ortho_diam_ai</th>\n",
       "      <th>mid_asc_aorta_ortho_diam_ai</th>\n",
       "      <th>prox_arch_ortho_diam_ai</th>\n",
       "      <th>mid_arch_ortho_diam_ai</th>\n",
       "      <th>prox_desc_ortho_diam_ai</th>\n",
       "      <th>mid_desc_ortho_diam_ai</th>\n",
       "      <th>diaphragm_ortho_diam_ai</th>\n",
       "      <th>abd_aorta_ortho_diam_ai</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  participant_id sin_vals_diam_ai sino_junc_diam_ai mid_asc_aorta_diam_ai  \\\n",
       "0            NaN              NaN               NaN                   NaN   \n",
       "\n",
       "  prox_arch_diam_ai mid_arch_diam_ai prox_desc_diam_ai mid_desc_diam_ai  \\\n",
       "0               NaN              NaN               NaN              NaN   \n",
       "\n",
       "  diaphragm_diam_ai abd_aorta_diam_ai sin_vals_ortho_diam_ai  \\\n",
       "0               NaN               NaN                    NaN   \n",
       "\n",
       "  sino_junc_ortho_diam_ai mid_asc_aorta_ortho_diam_ai prox_arch_ortho_diam_ai  \\\n",
       "0                     NaN                         NaN                     NaN   \n",
       "\n",
       "  mid_arch_ortho_diam_ai prox_desc_ortho_diam_ai mid_desc_ortho_diam_ai  \\\n",
       "0                    NaN                     NaN                    NaN   \n",
       "\n",
       "  diaphragm_ortho_diam_ai abd_aorta_ortho_diam_ai  \n",
       "0                     NaN                     NaN  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_aorta=pd.DataFrame(index=np.arange(1),columns=participant_id+diam_names+orth_names) \n",
    "df_aorta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6ba53aa7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>participant_id</th>\n",
       "      <th>ai_cacs_perf</th>\n",
       "      <th>ai_total_number_lesions</th>\n",
       "      <th>ai_total_artery_agatston</th>\n",
       "      <th>ai_total_artery_volume</th>\n",
       "      <th>ai_total_artery_mass</th>\n",
       "      <th>ai_lm_number_lesions</th>\n",
       "      <th>ai_lm_artery_agatston</th>\n",
       "      <th>ai_lm_artery_volume</th>\n",
       "      <th>ai_lm_artery_mass</th>\n",
       "      <th>...</th>\n",
       "      <th>ai_lad_artery_volume</th>\n",
       "      <th>ai_lad_artery_mass</th>\n",
       "      <th>ai_cx_number_lesions</th>\n",
       "      <th>ai_cx_artery_agatston</th>\n",
       "      <th>ai_cx_artery_volume</th>\n",
       "      <th>ai_cx_artery_mass</th>\n",
       "      <th>ai_rca_number_lesions</th>\n",
       "      <th>ai_rca_artery_agatston</th>\n",
       "      <th>ai_rca_artery_volume</th>\n",
       "      <th>ai_rca_artery_mass</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows × 22 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  participant_id ai_cacs_perf ai_total_number_lesions  \\\n",
       "0            NaN          NaN                     NaN   \n",
       "\n",
       "  ai_total_artery_agatston ai_total_artery_volume ai_total_artery_mass  \\\n",
       "0                      NaN                    NaN                  NaN   \n",
       "\n",
       "  ai_lm_number_lesions ai_lm_artery_agatston ai_lm_artery_volume  \\\n",
       "0                  NaN                   NaN                 NaN   \n",
       "\n",
       "  ai_lm_artery_mass  ... ai_lad_artery_volume ai_lad_artery_mass  \\\n",
       "0               NaN  ...                  NaN                NaN   \n",
       "\n",
       "  ai_cx_number_lesions ai_cx_artery_agatston ai_cx_artery_volume  \\\n",
       "0                  NaN                   NaN                 NaN   \n",
       "\n",
       "  ai_cx_artery_mass ai_rca_number_lesions ai_rca_artery_agatston  \\\n",
       "0               NaN                   NaN                    NaN   \n",
       "\n",
       "  ai_rca_artery_volume ai_rca_artery_mass  \n",
       "0                  NaN                NaN  \n",
       "\n",
       "[1 rows x 22 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_calcium=pd.DataFrame(index=np.arange(1),columns=participant_id+CACS_perf+total_ag+total_lm+total_lad+total_cx+total_rca)\n",
    "df_calcium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1da57307",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>participant_id</th>\n",
       "      <th>heart_vol_ai</th>\n",
       "      <th>cardiac_fat_vol_ai</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  participant_id heart_vol_ai cardiac_fat_vol_ai\n",
       "0            NaN          NaN                NaN"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_cardiac_fat=pd.DataFrame(index=np.arange(1),columns=participant_id+heart_fat)\n",
    "df_cardiac_fat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4c3e1864",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>participant_id</th>\n",
       "      <th>lung_vol_ai</th>\n",
       "      <th>lung_dens_ai_wl</th>\n",
       "      <th>lung_perc15_ai_wl</th>\n",
       "      <th>lung_dens_ai_lul</th>\n",
       "      <th>lung_perc15_ai_lul</th>\n",
       "      <th>lung_dens_ai_lll</th>\n",
       "      <th>lung_perc15_ai_lll</th>\n",
       "      <th>lung_dens_ai_rul</th>\n",
       "      <th>lung_perc15_ai_rul</th>\n",
       "      <th>lung_dens_ai_ml</th>\n",
       "      <th>lung_perc15_ai_ml</th>\n",
       "      <th>lung_dens_ai_rll</th>\n",
       "      <th>lung_perc15_ai_rll</th>\n",
       "      <th>ai_emphysema_quantification_complete</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  participant_id lung_vol_ai lung_dens_ai_wl lung_perc15_ai_wl  \\\n",
       "0            NaN         NaN             NaN               NaN   \n",
       "\n",
       "  lung_dens_ai_lul lung_perc15_ai_lul lung_dens_ai_lll lung_perc15_ai_lll  \\\n",
       "0              NaN                NaN              NaN                NaN   \n",
       "\n",
       "  lung_dens_ai_rul lung_perc15_ai_rul lung_dens_ai_ml lung_perc15_ai_ml  \\\n",
       "0              NaN                NaN             NaN               NaN   \n",
       "\n",
       "  lung_dens_ai_rll lung_perc15_ai_rll ai_emphysema_quantification_complete  \n",
       "0              NaN                NaN                                  NaN  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_emph=pd.DataFrame(index=np.arange(1),columns=participant_id+emphysema)\n",
    "df_emph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f66098ac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>participant_id</th>\n",
       "      <th>ai_nod_id1</th>\n",
       "      <th>ai_nod_id2</th>\n",
       "      <th>ai_nod_id3</th>\n",
       "      <th>ai_nod_id4</th>\n",
       "      <th>ai_nod_id5</th>\n",
       "      <th>ai_nod_id6</th>\n",
       "      <th>ai_nod_id7</th>\n",
       "      <th>ai_nod_id8</th>\n",
       "      <th>ai_nod_id9</th>\n",
       "      <th>...</th>\n",
       "      <th>pos1</th>\n",
       "      <th>pos2</th>\n",
       "      <th>pos3</th>\n",
       "      <th>pos4</th>\n",
       "      <th>pos5</th>\n",
       "      <th>pos6</th>\n",
       "      <th>pos7</th>\n",
       "      <th>pos8</th>\n",
       "      <th>pos9</th>\n",
       "      <th>pos10</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows × 51 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  participant_id ai_nod_id1 ai_nod_id2 ai_nod_id3 ai_nod_id4 ai_nod_id5  \\\n",
       "0            NaN        NaN        NaN        NaN        NaN        NaN   \n",
       "\n",
       "  ai_nod_id6 ai_nod_id7 ai_nod_id8 ai_nod_id9  ... pos1 pos2 pos3 pos4 pos5  \\\n",
       "0        NaN        NaN        NaN        NaN  ...  NaN  NaN  NaN  NaN  NaN   \n",
       "\n",
       "  pos6 pos7 pos8 pos9 pos10  \n",
       "0  NaN  NaN  NaN  NaN   NaN  \n",
       "\n",
       "[1 rows x 51 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_nodules=pd.DataFrame(index=np.arange(1),columns=participant_id+nod_ids+nod_volumes+nod_diam_2d+\n",
    "                        nod_diam_3d+nod_pos) \n",
    "# Also add 'nod_true_or_not' in the future, after running automation code to extract TP ids\n",
    "df_nodules"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b7a61b98",
   "metadata": {},
   "source": [
    "### Extract all information for CACS, aorta, cardiac fat, vertebra, emphysema and nodules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1aca871c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load dictionaries with participants and their AI files based on processing time AI outputs - Created with 'AI_timestamp.ipynb'\n",
    "\n",
    "with open(nodule_path,'rb') as f:\n",
    "    patient_and_nodules=pickle.load(f)\n",
    "with open(aorta_path,'rb') as f1:\n",
    "    patient_and_aorta=pickle.load(f1)\n",
    "with open(CACS_path,'rb') as f2:\n",
    "    patient_and_CACS=pickle.load(f2)\n",
    "with open(vertebra_path,'rb') as f3:\n",
    "    patient_and_vertebra=pickle.load(f3)\n",
    "with open(fat_path,'rb') as f4:\n",
    "    patient_and_fat=pickle.load(f4)\n",
    "with open(emph_path,'rb') as f5:\n",
    "    patient_and_emph=pickle.load(f5)\n",
    "    \n",
    "#Get lists of files for all participants in each category\n",
    "nodule_files_list=[item for item in list(patient_and_nodules.values())]  \n",
    "aorta_files_list=[item for item in list(patient_and_aorta.values())]    \n",
    "CACS_files_list=[item for item in list(patient_and_CACS.values())]\n",
    "vertebra_files_list=[item for item in list(patient_and_vertebra.values())]\n",
    "fat_files_list=[item for item in list(patient_and_fat.values())]\n",
    "emph_files_list=[item for item in list(patient_and_emph.values())]\n",
    "\n",
    "#In case that we don't want to extract some of the above comment the above line and activate the corresponding one below\n",
    "# nodule_files_list=[]\n",
    "# aorta_files_list=[]\n",
    "# CACS_files_list=[]\n",
    "# vertebra_files_list=[]\n",
    "# fat_files_list=[]\n",
    "# emph_files_list=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0f5f8f45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of aorta files is 1368\n",
      "Total number of CACS files is 3651\n",
      "Total number of vertebra files is 1654\n",
      "Total number of nodule files is 334\n",
      "Total number of cardiac fat files is 1647\n",
      "Total number of emphysema files is 1654\n"
     ]
    }
   ],
   "source": [
    "print('Total number of aorta files is',len(aorta_files_list))\n",
    "print('Total number of CACS files is',len(CACS_files_list))\n",
    "print('Total number of vertebra files is',len(vertebra_files_list))\n",
    "print('Total number of nodule files is',len(nodule_files_list))\n",
    "print('Total number of cardiac fat files is',len(fat_files_list))\n",
    "print('Total number of emphysema files is',len(emph_files_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a7252683",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_measurements(AI_path=AI_path, #Path with AI txt files with information to extract\n",
    "                    \n",
    "                    #Paths of files having information for each of the nodules, aorta etc. \n",
    "                    nodule_files_list=nodule_files_list,\n",
    "                    aorta_files_list=aorta_files_list,\n",
    "                    CACS_files_list=CACS_files_list,\n",
    "                    vertebra_files_list=vertebra_files_list,\n",
    "                    fat_files_list=fat_files_list,\n",
    "                    emph_files_list=emph_files_list,\n",
    "\n",
    "                    #Empty dictionaries having as column names the attributes to be extracted for each of the above files\n",
    "                    df_nodules=df_nodules,\n",
    "                    df_aorta=df_aorta,\n",
    "                    df_calcium=df_calcium,\n",
    "                    df_vertebra=df_vertebra,\n",
    "                    df_cardiac_fat=df_cardiac_fat,\n",
    "                    df_emph=df_emph\n",
    "                    ):\n",
    "    \n",
    "    'Gets the path of the AI txt files, along with the paths of measurements that we want to extract (nodules, aorta,'\n",
    "    'CACS,vertebra, and cardiac_fat) along with the empty dataframes with the columns that we want to fill and import in REDCap.'\n",
    "    'Returns these dataframes filled with the information of the AI txt files'\n",
    "    'Dataframes are returned in the following order: df_nodules, df_aorta, df_calcium, df_vertebra, df_cardiac_fat, df_emph'\n",
    "\n",
    "    # #High Emphysema - Just used to get a list of participants with high emphysema, will not be uploaded\n",
    "    # high_emphysema_pats=[] #Keep track of participants with high emphysema (>20 specified below)\n",
    "   \n",
    "\n",
    "    for textfile in os.listdir(AI_path): #Loop over folder with AI txt files with information\n",
    "        \n",
    "        if (textfile in nodule_files_list or textfile in aorta_files_list or textfile in CACS_files_list\n",
    "        or textfile in vertebra_files_list or textfile in fat_files_list or textfile in emph_files_list): \n",
    "            #If the txt file exists in any of the files for which we want to extract information\n",
    "\n",
    "\n",
    "            with open (AI_path+'/'+textfile) as f:\n",
    "                lines=f.readlines() #Read all lines in txt file\n",
    "\n",
    "                if len(lines[2])==17 or 'imalife_' in lines[2].lower(): #10 for 'PatientID:', 6 for the actual numbers, and 1 extra character for newline\n",
    "\n",
    "# #                     Just for debugging\n",
    "#                     if \"PatientID:435703\" in lines[2]:\n",
    "#                         print(\"File for participant 435703 is\",textfile)\n",
    "    \n",
    "    \n",
    "##########For vertebra measurements below\n",
    "\n",
    "                    if textfile in vertebra_files_list: #If txt files in list of files with vertebra measurements\n",
    "        \n",
    "                        try: #Since file might not be processed correctly by AI\n",
    "                            #List of columns of the dataframe we want to import in REDCap\n",
    "                            columns_vertebra=[participant_id+vert_t1+vert_t2+vert_t3+vert_t4+vert_t5+vert_t6+vert_t7+vert_t8+vert_t9+vert_t10+vert_t11+vert_t12+vert_l1+vert_c7]\n",
    "\n",
    "                            #Initialize empty dataframe - similar to series, just for one participant to be appended below\n",
    "                            series_vertebra=pd.DataFrame(index=np.arange(1),columns=participant_id+vert_t1+vert_t2+\n",
    "                                                         vert_t3+vert_t4+vert_t5+vert_t6+vert_t7+vert_t8+vert_t9+\n",
    "                                                         vert_t10+vert_t11+vert_t12+vert_l1+vert_c7)\n",
    "\n",
    "                            series_vertebra['participant_id']=int(lines[2][-7:]) #Add participant id\n",
    "\n",
    "                            vert_measurements=[s for s in lines if '{\"label\":' in s] #Vertebra measurements\n",
    "\n",
    "                            vert_stats=[s for s in lines if 'HU:' in s] #HU value measurements\n",
    "                            \n",
    "                            vert_errors=eval([s for s in lines if '\"errorInfo\":' in s][0]) #Errors by AI\n",
    "\n",
    "                            #Check error outputs by AI in vertebra measurement and print file and participant for those cases\n",
    "                            for error in vert_errors:\n",
    "                                if error['errorInfo']!='': #When there is an error\n",
    "                                    print('There are errors for patient {} in file {}'.format(int(lines[2][-7:]),textfile))\n",
    "\n",
    "                            if len(vert_measurements)!=[]: #When there are vertebra measurements\n",
    "                                \n",
    "                                vert_inside_measurements=eval([s for s in lines if '{\"label\":' in s][0]) #Get dictionary of them\n",
    "\n",
    "                                for vert_measurement in vert_inside_measurements: #Loop over keys of dictionary with measurements\n",
    "\n",
    "                                    label=vert_measurement['label']\n",
    "                                    label_lower=label.lower() #Get name of vertebrae and set it to lowercase to be used in attributes below\n",
    "                                    \n",
    "                                    #Extract values from the txt file for each vertebra\n",
    "                                    volume=vert_measurement['volumeInCm3']\n",
    "                                    posterior_height=vert_measurement['posteriorHeightDiameter']['lengthInMm']\n",
    "                                    anterior_height=vert_measurement['anteriorHeightDiameter']['lengthInMm']\n",
    "                                    mid_height=vert_measurement['midHeightDiameter']['lengthInMm']\n",
    "\n",
    "                                    #Round the above values to two digits and add them to a dataframe to be appended to the full one below                       \n",
    "                                    series_vertebra['vert_volume_'+label_lower]=round(volume,2)\n",
    "                                    series_vertebra['vert_post_height_'+label_lower]=round(posterior_height,2)\n",
    "                                    series_vertebra['vert_ant_height_'+label_lower]=round(anterior_height,2)\n",
    "                                    series_vertebra['vert_mid_height_'+label_lower]=round(mid_height,2)\n",
    "\n",
    "\n",
    "                            if vert_stats!=[]: #When there are HU value measurements\n",
    "                                \n",
    "                                for vertebrae in vert_stats: #For each of those HU value measurements in the dictionary\n",
    "                                    \n",
    "                                    label_and_hu=vertebrae.split('Label: ')[1].split(',HU: ') #Get label and HU value\n",
    "                                    label=label_and_hu[0] #Label is the first element\n",
    "                                    label_lower=label.lower() #Lowercase to be used in attribute name below\n",
    "                                    hu=label_and_hu[1][:-1] #Get HU value for the above label\n",
    "\n",
    "                                    series_vertebra['vert_mean_hu_'+label_lower]=round(float(hu),2) #Round it and add it to dataframe\n",
    "\n",
    "                                    \n",
    "                            #Append above participant to the final dataframe        \n",
    "                            df_vertebra=df_vertebra.append(series_vertebra,ignore_index=True)\n",
    "                            \n",
    "                            #If more attributes are added (eg. l2 etc.) remove those at the end since not needed\n",
    "                            df_vertebra=df_vertebra.iloc[:,:len(columns_vertebra[0])] \n",
    "                            \n",
    "                            \n",
    "                        except: #If there are errors print the participant that wasn't processed correctly and its AI file\n",
    "                            try:\n",
    "                                print('Vertebra measurements not processed for participant {} in file {}'.format(int(lines[2][-7:]),textfile))\n",
    "                                # print(traceback.format_exc())\n",
    "                            except: #This means problem with participant id - Probably not a 6 digit number\n",
    "                                print(\"Problem with participant_id in vertebra measurements of file {}\".format(textfile))    \n",
    "                            # print(traceback.format_exc())\n",
    "                            print('\\n')               \n",
    "\n",
    "\n",
    "##########For cardiac fat and heart volume measurements below\n",
    "\n",
    "                    if textfile in fat_files_list: #If txt files in list of files with cardiac fat measurements\n",
    "        \n",
    "                        try: #In case there are errors\n",
    "                            \n",
    "                            #Initialize dataframe similar as above to be filled with just one row and appended to the final one\n",
    "                            series_fat=pd.DataFrame(index=np.arange(1),columns=participant_id+heart_fat)\n",
    "\n",
    "                            series_fat['participant_id']=int(lines[2][-7:]) #Add participant id\n",
    "\n",
    "                            #Extract heart volume, round it to 2 digits, and append it to the above dataframe\n",
    "                            heart_measurements=[s for s in lines if 'Heart Volume (mm3):' in s]\n",
    "                            series_fat['heart_vol_ai']=round(float(heart_measurements[0].split('Heart Volume (mm3):')[-1][1:-1]),2)\n",
    "                        \n",
    "                            #Extract cardiac fat measurement, round it to 2 digits, and append it to the above dataframe\n",
    "                            fat_measurements=[s for s in lines if 'Absolute Volume (ml):' in s]\n",
    "                            series_fat['cardiac_fat_vol_ai']=round(float(fat_measurements[0].split('Absolute Volume (ml):')[-1][1:-1]),2)\n",
    "\n",
    "                            #Extract mean for cardiac fat HU\n",
    "                            mean_HU=[s for s in lines if 'Mean :' in s]\n",
    "                            series_fat['cardiac_fat_hu']=round(float(mean_HU[0].split('Mean :')[-1][1:-1]),2)\n",
    "\n",
    "                            #Extract std for cardiac fat HU\n",
    "                            std_HU=[s for s in lines if 'Standard Deviation :' in s]\n",
    "                            series_fat['cardiac_fat_hu_sd']=round(float(std_HU[0].split('Standard Deviation :')[-1][1:-1]),2)\n",
    "\n",
    "                            #At the end, append the above participant and its measurements to the final dataframe    \n",
    "                            df_cardiac_fat=df_cardiac_fat.append(series_fat,ignore_index=True)\n",
    "        \n",
    "                        except: #If errors, print participant for whom they occurred and its AI file\n",
    "                            try:\n",
    "                                print('Cardiac fat and/or heart measurements not processed for participant {} in file {}'.format(int(lines[2][-7:]),textfile))\n",
    "                                # print(traceback.format_exc())\n",
    "                            except:\n",
    "                                print(\"Problem with participant_id in cardiac_fat measurements of file {}\".format(textfile))\n",
    "                            # print(traceback.format_exc())\n",
    "                            print('\\n')\n",
    "        \n",
    "\n",
    "##########For emphysema measurements below\n",
    "\n",
    "                    if textfile in emph_files_list: #If txt files in list of files with emphysema measurements\n",
    "        \n",
    "                        try: #In case there are errors\n",
    "                            \n",
    "                            #Initialize dataframe similar as above to be filled with just one row and appended to the final one\n",
    "                            series_emph=pd.DataFrame(index=np.arange(1),columns=participant_id+emphysema)\n",
    "\n",
    "                            series_emph['participant_id']=int(lines[2][-7:]) #Add participant id\n",
    "\n",
    "                            #Extract emphysema and append it to the above dataframe\n",
    "                            emph_measurements=[s for s in lines if ':950 - Lobe:' in s]                          \n",
    "                            \n",
    "                            for lobe in emph_measurements: #Loop over all measurements at 950 HU threshold\n",
    "                                #Depending on the lobe add measurements to the corresponding column of dataframe\n",
    "                                \n",
    "                                if 'Lobe:WHOLE' in lobe: #Whole lung\n",
    "                                    if float(lobe.split('Perc15:')[-1][:-1])!=0:\n",
    "                                        series_emph['lung_vol_ai']=float(lobe.split('outVolume:')[1].split(' - ')[0])\n",
    "                                        series_emph['lung_dens_ai_wl']=float(lobe.split('outLAAPercent:')[1].split(' - ')[0])\n",
    "                                        series_emph['lung_perc15_ai_wl']=float(lobe.split('Perc15:')[-1][:-1])\n",
    "\n",
    "                                if 'Lobe:LU' in lobe: #Left Upper lobe\n",
    "                                    if float(lobe.split('Perc15:')[-1][:-1])!=0:\n",
    "                                        series_emph['lung_dens_ai_lul']=float(lobe.split('outLAAPercent:')[1].split(' - ')[0])\n",
    "                                        series_emph['lung_perc15_ai_lul']=float(lobe.split('Perc15:')[-1][:-1])\n",
    "\n",
    "                                if 'Lobe:LL' in lobe: #Left Lower lobe\n",
    "                                    if float(lobe.split('Perc15:')[-1][:-1])!=0:\n",
    "                                        series_emph['lung_dens_ai_lll']=float(lobe.split('outLAAPercent:')[1].split(' - ')[0])\n",
    "                                        series_emph['lung_perc15_ai_lll']=float(lobe.split('Perc15:')[-1][:-1])\n",
    "\n",
    "                                if 'Lobe:RU' in lobe: #Right Upper lobe\n",
    "                                    if float(lobe.split('Perc15:')[-1][:-1])!=0:\n",
    "                                        series_emph['lung_dens_ai_rul']=float(lobe.split('outLAAPercent:')[1].split(' - ')[0])\n",
    "                                        series_emph['lung_perc15_ai_rul']=float(lobe.split('Perc15:')[-1][:-1])\n",
    "\n",
    "                                if 'Lobe:RM' in lobe: #Right Middle lobe\n",
    "                                    if float(lobe.split('Perc15:')[-1][:-1])!=0:\n",
    "                                        series_emph['lung_dens_ai_ml']=float(lobe.split('outLAAPercent:')[1].split(' - ')[0])\n",
    "                                        series_emph['lung_perc15_ai_ml']=float(lobe.split('Perc15:')[-1][:-1])\n",
    "\n",
    "                                if 'Lobe:RL' in lobe: #Right Lower lobe\n",
    "                                    if float(lobe.split('Perc15:')[-1][:-1])!=0:\n",
    "                                        series_emph['lung_dens_ai_rll']=float(lobe.split('outLAAPercent:')[1].split(' - ')[0])\n",
    "                                        series_emph['lung_perc15_ai_rll']=float(lobe.split('Perc15:')[-1][:-1])\n",
    "                            \n",
    "                            \n",
    "                            series_emph['ai_emphysema_quantification_complete']=1 #Set this to 'unverified' to check manually after uploading\n",
    "\n",
    "                            #At the end, append the above participant and its measurements to the final dataframe    \n",
    "                            df_emph=df_emph.append(series_emph,ignore_index=True)\n",
    "                            \n",
    "                            # #Check for participants with high percentage of emphysema (>20% in any lobe) - Used for checks, will not be uploaded\n",
    "                            # if (series_emph['lung_dens_ai_wl'].values[0]>=20 or \n",
    "                            #     series_emph['lung_dens_ai_lul'].values[0]>=20 or \n",
    "                            #     series_emph['lung_dens_ai_lll'].values[0]>=20 or \n",
    "                            #     series_emph['lung_dens_ai_rul'].values[0]>=20 or \n",
    "                            #     series_emph['lung_dens_ai_ml'].values[0]>=20 or \n",
    "                            #     series_emph['lung_dens_ai_rll'].values[0]>=20):\n",
    "                                \n",
    "                            #     high_emphysema_pats.append(int(lines[2][-7:])) #Add them to list\n",
    "                                \n",
    "       \n",
    "                        except: #If errors here print participant for whom they occurred and its AI file\n",
    "                            try:\n",
    "                                print('Emphysema measurements not procesed for participant {} in file {}'.format(int(lines[2][-7:]),textfile))\n",
    "                                # print(traceback.format_exc())\n",
    "                            except:\n",
    "                                print(\"Problem with participant_id in emphysema measurements of file {}\".format(textfile))\n",
    "                            # print(traceback.format_exc())\n",
    "                            print('\\n')\n",
    "\n",
    "\n",
    "##########For aortic measurements below\n",
    "\n",
    "                    if textfile in aorta_files_list: #If txt files in list of files with aorta measurements\n",
    "\n",
    "                        pat=[] #To temporarily save a patient ID and add it to dataframe\n",
    "                        diam=[] #To store all 9 diameter values\n",
    "                        orth_diam=[] #To store all 9 orthogonal diameter values\n",
    "\n",
    "                        try: #In case of errors\n",
    "            \n",
    "                            measurements=[s for s in lines if '{\"id\":' in s] #All aortic diameter measurements\n",
    "\n",
    "                            if len(measurements)!=0: #If we have measurements\n",
    "\n",
    "                                #Split to 9 elements of list - one for each measurement\n",
    "                                inside_measurements=eval([s for s in lines if '{\"id\":' in s][0]) \n",
    "\n",
    "                                if len(inside_measurements)!=9: #We should not get in here\n",
    "                                    print(\"We don't have 9 aorta measurements for file {}. We only have {}\".format(textfile,len(inside_measurements)))\n",
    "\n",
    "                                else: #If we have 9 aortic measurements\n",
    "                                    pat.append(int(lines[2][-7:])) #Add that patient to a temporary list\n",
    "\n",
    "                                    for i in range(9): #Loop over these measurements to extract information\n",
    "                                        further_inside_measur=inside_measurements[i] #Get line with specific measurement\n",
    "                                        diam.append(round(further_inside_measur['diameter']['lengthInMm'],2)) #Add diameter to list\n",
    "                                        orth_diam.append(round(further_inside_measur['orthogonal_diameter']['lengthInMm'],2)) #Add orthogonal diameter to list\n",
    "\n",
    "                                    series=[pat,diam,orth_diam] #Create list of lists with attributes to be filled in dataframe\n",
    "                                    series_flat=[x_in for x in series for x_in in x] #Convert list of lists to list\n",
    "                                    series_flat[0]=int(series_flat[0]) #Convert patient id to integer           \n",
    "\n",
    "                                    series_df=pd.Series(series_flat,index=df_aorta.columns) #Create a series object from the above\n",
    "\n",
    "                                    #At the end, append the above participant and its measurements to the final dataframe    \n",
    "                                    df_aorta=df_aorta.append(series_df,ignore_index=True)\n",
    "\n",
    "                            #Not needed to have an else statement here since it will also give all files not sent for aorta (eg. sent only for nodules)                                        \n",
    "\n",
    "                        except: \n",
    "                            #Empty lists again since some of them may have been filled before error occured\n",
    "                            pat=[]\n",
    "                            diam=[]\n",
    "                            orth_diam=[]\n",
    "                            try:\n",
    "                                print('Error in aorta measurements for participant {} in file {}'.format(int(lines[2][-7:]),textfile))\n",
    "                            except:\n",
    "                                print(\"Problem with participant_id in aorta measurements of file {}\".format(textfile))\n",
    "                            # print(traceback.format_exc())\n",
    "                            print('\\n')\n",
    "\n",
    "\n",
    "##########For calcium score measurements below    \n",
    "\n",
    "                    if textfile in CACS_files_list: #If txt files in list of files with CACS measurements\n",
    "\n",
    "                        try: #Since we may have errors in some files\n",
    "                            all_measures=[s for s in lines if 'Coronaary branch measurements:' in s] \n",
    "\n",
    "                            if len(all_measures)!=0: #If we have branch measurements\n",
    "\n",
    "                                #create empty series with required column names to be added to REDCap\n",
    "                                series_calcium=pd.DataFrame(index=np.arange(1),columns=participant_id+CACS_perf+total_ag+total_lm+total_lad+total_cx+total_rca) \n",
    "                                series_calcium['participant_id']=int(lines[2][-7:]) #Add participant id\n",
    "\n",
    "                                #Extract the total agatston score, num of lesions, volume and mass for each branch\n",
    "                                tot_aga=[s.split('\\t') for s in lines if 'TOT' in s]\n",
    "                                tot_lm=[s.split('\\t') for s in lines if 'LM' in s]\n",
    "                                tot_lad=[s.split('\\t') for s in lines if 'LAD' in s]\n",
    "                                tot_cx=[s.split('\\t') for s in lines if 'CX' in s]\n",
    "                                tot_rca=[s.split('\\t') for s in lines if 'RCA' in s]\n",
    "\n",
    "                                series_calcium['ai_cacs_perf']='1' #Set this variable to 1 to denote that we will add measurements - REDCap needs that\n",
    "\n",
    "                                series_calcium['ai_total_artery_mass']=tot_aga[0][0].split(' ')[0]\n",
    "                                series_calcium['ai_total_artery_agatston']=tot_aga[0][3].split(' ')[0]\n",
    "                                series_calcium['ai_total_number_lesions']=tot_aga[0][4].split(' ')[0]\n",
    "                                series_calcium['ai_total_artery_volume']=tot_aga[0][6].split(' ')[0]\n",
    "\n",
    "                                series_calcium['ai_lm_artery_mass']=tot_lm[0][0].split(' ')[0]\n",
    "                                series_calcium['ai_lm_artery_agatston']=tot_lm[0][3].split(' ')[0]\n",
    "                                series_calcium['ai_lm_number_lesions']=tot_lm[0][4].split(' ')[0]\n",
    "                                series_calcium['ai_lm_artery_volume']=tot_lm[0][6].split(' ')[0]\n",
    "\n",
    "                                series_calcium['ai_lad_artery_mass']=tot_lad[0][0].split(' ')[0]\n",
    "                                series_calcium['ai_lad_artery_agatston']=tot_lad[0][3].split(' ')[0]\n",
    "                                series_calcium['ai_lad_number_lesions']=tot_lad[0][4].split(' ')[0]\n",
    "                                series_calcium['ai_lad_artery_volume']=tot_lad[0][6].split(' ')[0]\n",
    "\n",
    "                                series_calcium['ai_cx_artery_mass']=tot_cx[0][0].split(' ')[0]                            \n",
    "                                series_calcium['ai_cx_artery_agatston']=tot_cx[0][3].split(' ')[0]\n",
    "                                series_calcium['ai_cx_number_lesions']=tot_cx[0][4].split(' ')[0]\n",
    "                                series_calcium['ai_cx_artery_volume']=tot_cx[0][6].split(' ')[0]\n",
    "\n",
    "                                series_calcium['ai_rca_artery_mass']=tot_rca[0][0].split(' ')[0]\n",
    "                                series_calcium['ai_rca_artery_agatston']=tot_rca[0][3].split(' ')[0]\n",
    "                                series_calcium['ai_rca_number_lesions']=tot_rca[0][4].split(' ')[0]\n",
    "                                series_calcium['ai_rca_artery_volume']=tot_rca[0][6].split(' ')[0]                            \n",
    "\n",
    "                                #Append that information to the final df with CACS measurements\n",
    "                                df_calcium=df_calcium.append(series_calcium,ignore_index=True)\n",
    "\n",
    "                        except:\n",
    "                            try:\n",
    "                                print('Error in CACS measurements for participant {} in file {}'.format(int(lines[2][-7:]),textfile))\n",
    "                            except:\n",
    "                                print(\"Problem with participant_id in CACS measurements of file {}\".format(textfile))\n",
    "                            # print(traceback.format_exc())\n",
    "                            print('\\n')\n",
    "\n",
    "\n",
    "##########For lung nodules measurements below\n",
    "\n",
    "                    if textfile in nodule_files_list: #If txt files in list of files with nodule measurements\n",
    "\n",
    "                        pat_nod=[] #To temporarily save a patient ID and add it to dataframe\n",
    "                        nod_ids_new=[] #To be filled with nodule ids\n",
    "                        vols=[] #To be filled with volume of nodules\n",
    "                        diam_2d=[] #To be filled with the 2D diameter of nodules\n",
    "                        diam_3d=[] #To be filled with the 3D diameter of nodules\n",
    "\n",
    "                        #The following needed only for automation algorithm - Cannot be imported directly to RedCap\n",
    "                        slice_pos=[] #To be filled with the slice positions in which the nodules could be found\n",
    "                        # nod_type=[] #To be filled with the nodule type (solid, subsolid etc.)\n",
    "\n",
    "                        try:\n",
    "                            lesions_lung=[s for s in lines if 'Lesion :' in s] #Get information about nodules\n",
    "\n",
    "                            if len(lesions_lung)!=0: #If we have lung nodule measurements\n",
    "\n",
    "                                pat_nod.append(int(lines[2][-7:])) #Add participant_id to list\n",
    "\n",
    "                                #Loop over each line and add information for nodule_id, volume, slice, diameters (and nodule type) to lists\n",
    "                                for ind in range(len(lesions_lung)): \n",
    "\n",
    "                                    nod_ids_new.append(ind+1)\n",
    "                                    vols.append(float(lesions_lung[ind].split(':')[3].split(',')[0])) \n",
    "                                    slice_pos.append(float(lesions_lung[ind].split(':')[2].split(',')[2][:-1])) \n",
    "                                    diam_2d.append(float(lesions_lung[ind].split(':')[4].split(',')[0]))\n",
    "                                    diam_3d.append(float(lesions_lung[ind].split(':')[5].split(',')[0]))\n",
    "                                    # nod_type.append(lesions_lung[ind].split(':')[6].strip())\n",
    "\n",
    "                                #For the remaining nodules (until 10) fill empty strings to list instead of 'np.nan'\n",
    "                                for i in range(10-len(nod_ids_new)): \n",
    "                                    nod_ids_new.append('')\n",
    "                                    vols.append('')\n",
    "                                    slice_pos.append('')\n",
    "                                    diam_2d.append('')\n",
    "                                    diam_3d.append('')\n",
    "                                    # nod_type.append('')\n",
    "\n",
    "                                series_nod=[pat_nod,nod_ids_new,vols,diam_2d,diam_3d,slice_pos] #Create list of lists with attributes to be filled in dataframe\n",
    "                                series_flat_nod=[x_in for x in series_nod for x_in in x] #Convert list of lists to list                                \n",
    "                                series_flat_nod[0]=int(series_flat_nod[0]) #Convert patient id to integer\n",
    "                                \n",
    "                                series_df_nod=pd.Series(series_flat_nod,index=df_nodules.columns) #Create a series object from the above\n",
    "\n",
    "                                df_nodules=df_nodules.append(series_df_nod,ignore_index=True) #Add it to df\n",
    "\n",
    "                            #Ensure that all patients (send or not for nodules) will exist in df, even with nan values\n",
    "                            elif len(lesions_lung)==0: #If empty fill empty values for that patient            \n",
    "                            #This happens when no nodules detected or when scan sent for aorta or other measurements and not for nodules\n",
    "\n",
    "                                    if lines[2][-7:] not in (np.unique(df_nodules['participant_id'])): #If patient not in df\n",
    "\n",
    "                                        series_df_fill=pd.Series(index=df_nodules.columns,dtype='object') #Create a series object from the above\n",
    "\n",
    "                                        series_df_fill['participant_id']=int(lines[2][-7:]) #Add patient to above series\n",
    "\n",
    "                                        #Add patient with software version to df\n",
    "                                        df_nodules=df_nodules.append(series_df_fill,ignore_index=True) \n",
    "\n",
    "\n",
    "                        except:\n",
    "                            try:\n",
    "                                print('Error in nodule measurements for participant {} in file {}'.format(int(lines[2][-7:]),textfile))\n",
    "                            except:\n",
    "                                print(\"Problem with participant_id in nodule measurements of file {}\".format(textfile))\n",
    "                            # print(traceback.format_exc())\n",
    "                            print('\\n')\n",
    "\n",
    "                            #Empty lists since there may not be emptied if error occurs\n",
    "                            pat_nod=[]\n",
    "                            temp_soft_vers=[]\n",
    "                            nod_ids_new=[]\n",
    "                            vols=[]\n",
    "                            diam_2d=[]\n",
    "                            diam_3d=[]\n",
    "                            slice_pos=[]\n",
    "                            # nod_type=[]\n",
    "\n",
    "                            \n",
    "\n",
    "                #If we have an invalid participant id - Might only get in here if participant was sent in a very early version of Siemens AI Rad Companion                          \n",
    "                else:\n",
    "                    print(\"Invalid patient {} found in file {}\".format(lines[2],textfile))\n",
    "                    print('\\n')\n",
    "      \n",
    "    #Save list of participants with high emphysema (>20% in any lobe) in excel\n",
    "#     high_emphysema_df=pd.DataFrame(high_emphysema_pats,columns=['participant_id'])\n",
    "#     high_emphysema_df.to_excel('high_emphysema_participants.xlsx',index=False)\n",
    "\n",
    "    return df_nodules, df_aorta, df_calcium, df_vertebra, df_cardiac_fat, df_emph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3b762116",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are errors for patient 325373 in file 1.2.276.0.28.3.345049594267.42.10032.20220925082922036.txt\n",
      "Vertebra measurements not processed for participant 565033 in file 1.2.276.0.28.3.345049594267.42.10156.20220930193802000.txt\n",
      "\n",
      "\n",
      "There are errors for patient 198041 in file 1.2.276.0.28.3.345049594267.42.1668.20220923112410000.txt\n",
      "There are errors for patient 126676 in file 1.2.276.0.28.3.345049594267.42.1988.20220914133717000.txt\n",
      "There are errors for patient 339537 in file 1.2.276.0.28.3.345049594267.42.2204.20220925125353036.txt\n",
      "There are errors for patient 339537 in file 1.2.276.0.28.3.345049594267.42.2204.20220925125353036.txt\n",
      "There are errors for patient 339537 in file 1.2.276.0.28.3.345049594267.42.2204.20220925125353036.txt\n",
      "Vertebra measurements not processed for participant 126961 in file 1.2.276.0.28.3.345049594267.42.2804.20220914144515036.txt\n",
      "\n",
      "\n",
      "There are errors for patient 134516 in file 1.2.276.0.28.3.345049594267.42.3052.20220921133240036.txt\n",
      "Vertebra measurements not processed for participant 218251 in file 1.2.276.0.28.3.345049594267.42.3184.20220923174736000.txt\n",
      "\n",
      "\n",
      "Vertebra measurements not processed for participant 389315 in file 1.2.276.0.28.3.345049594267.42.3264.20220926095226036.txt\n",
      "\n",
      "\n",
      "Vertebra measurements not processed for participant 704912 in file 1.2.276.0.28.3.345049594267.42.3816.20221002234818036.txt\n",
      "\n",
      "\n",
      "There are errors for patient 998858 in file 1.2.276.0.28.3.345049594267.42.3844.20221007140336036.txt\n",
      "There are errors for patient 131253 in file 1.2.276.0.28.3.345049594267.42.392.20220917001926036.txt\n",
      "Vertebra measurements not processed for participant 264808 in file 1.2.276.0.28.3.345049594267.42.4132.20220924120057036.txt\n",
      "\n",
      "\n",
      "There are errors for patient 585315 in file 1.2.276.0.28.3.345049594267.42.4220.20221001040148036.txt\n",
      "There are errors for patient 134443 in file 1.2.276.0.28.3.345049594267.42.4504.20220921115715036.txt\n",
      "There are errors for patient 793813 in file 1.2.276.0.28.3.345049594267.42.4516.20221004131303036.txt\n",
      "There are errors for patient 220735 in file 1.2.276.0.28.3.345049594267.42.4620.20220923185812036.txt\n",
      "Vertebra measurements not processed for participant 448877 in file 1.2.276.0.28.3.345049594267.42.4804.20220929002537036.txt\n",
      "\n",
      "\n",
      "There are errors for patient 134196 in file 1.2.276.0.28.3.345049594267.42.4924.20220919220557000.txt\n",
      "There are errors for patient 134196 in file 1.2.276.0.28.3.345049594267.42.4924.20220919220557000.txt\n",
      "There are errors for patient 629403 in file 1.2.276.0.28.3.345049594267.42.5044.20221001223638000.txt\n",
      "There are errors for patient 479623 in file 1.2.276.0.28.3.345049594267.42.5220.20220929114954036.txt\n",
      "There are errors for patient 177919 in file 1.2.276.0.28.3.345049594267.42.5924.20220923041830000.txt\n",
      "There are errors for patient 132614 in file 1.2.276.0.28.3.345049594267.42.6000.20220918083612036.txt\n",
      "There are errors for patient 448759 in file 1.2.276.0.28.3.345049594267.42.6152.20220929000916036.txt\n",
      "There are errors for patient 140981 in file 1.2.276.0.28.3.345049594267.42.7356.20220922155110036.txt\n",
      "There are errors for patient 872935 in file 1.2.276.0.28.3.345049594267.42.8104.20221005182754036.txt\n",
      "There are errors for patient 872935 in file 1.2.276.0.28.3.345049594267.42.8104.20221005182754036.txt\n",
      "There are errors for patient 909471 in file 1.2.276.0.28.3.345049594267.42.8116.20221006103059036.txt\n",
      "There are errors for patient 963840 in file 1.2.276.0.28.3.345049594267.42.8208.20221007042441000.txt\n",
      "There are errors for patient 963840 in file 1.2.276.0.28.3.345049594267.42.8208.20221007042441000.txt\n",
      "There are errors for patient 288366 in file 1.2.276.0.28.3.345049594267.42.8260.20220924183243000.txt\n",
      "Vertebra measurements not processed for participant 724638 in file 1.2.276.0.28.3.345049594267.42.8736.20221003070003000.txt\n",
      "\n",
      "\n",
      "There are errors for patient 645085 in file 1.2.276.0.28.3.345049594267.42.8816.20221002053658036.txt\n",
      "There are errors for patient 626768 in file 1.2.276.0.28.3.345049594267.42.8832.20221001210432036.txt\n",
      "There are errors for patient 979548 in file 1.2.276.0.28.3.345049594267.42.8840.20221007082918036.txt\n",
      "There are errors for patient 111884 in file 1.2.276.0.28.3.345049594267.42.9524.20220914095157036.txt\n",
      "Vertebra measurements not processed for participant 623450 in file 1.2.276.0.28.3.345049594267.42.9620.20221001193039036.txt\n",
      "\n",
      "\n",
      "There are errors for patient 987624 in file 1.2.276.0.28.3.345049594267.42.9764.20221007093714000.txt\n"
     ]
    }
   ],
   "source": [
    "# %%capture cap --no-stderr\n",
    "# #The above command to save output of cell in a txt file - here the errors \n",
    "#If used, should be accompanied by the command in the next cell (activate it)\n",
    "\n",
    "df_nodules,df_aorta,df_calcium,df_vertebra,df_cardiac_fat,df_emph=get_measurements()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c469b988",
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('errors_all.txt','w') as f: #Save output of above cell to txt file\n",
    "#     f.write(cap.stdout)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d89d8bb4",
   "metadata": {},
   "source": [
    "### CACS information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "979f7078",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_calcium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8a3bdb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_calcium=df_calcium.dropna() #To delete all rows with nans - First row created when adding empty series above\n",
    "df_calcium.reset_index(drop=True,inplace=True) #reset index and drop index column\n",
    "df_calcium.participant_id=df_calcium.participant_id.astype(int) #Convert first column with participant IDs to integers\n",
    "df_calcium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8217abe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert len(np.unique(df_calcium.participant_id))==len(df_calcium.participant_id) #Confirm that we only have unique patients in df_calcium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5dc7ca51",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_calcium.to_csv('CACS_AI_21-4.csv',index=False) #Save file to csv\n",
    "df_calcium.to_excel('CACS_AI_21-4.xlsx',index=False) #Save file to excel"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e5ba79c3",
   "metadata": {},
   "source": [
    "### Aorta information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3533089f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_aorta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "262d8d2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_aorta=df_aorta.dropna() #To delete all rows with nans - First row created when adding empty series above\n",
    "df_aorta.reset_index(drop=True,inplace=True) #reset index and drop index column\n",
    "df_aorta.participant_id=df_aorta.participant_id.astype(int) #Convert first column with participant IDs to integers\n",
    "df_aorta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "8f4c4fff",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert len(np.unique(df_aorta.participant_id))==len(df_aorta.participant_id) #Confirm that we only have unique patients in df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "8086760c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_aorta.to_csv('aorta_RedCap_21-4.csv',index=False) #Save file to csv\n",
    "df_aorta.to_excel('aorta_RedCap_21-4.xlsx',index=False) #Save file to excel"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "01ac9394",
   "metadata": {},
   "source": [
    "### Vertebra information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42a33837",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_vertebra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8b3232d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_vertebra=df_vertebra.iloc[1:] #Remove first row with nan created above\n",
    "#There are many more rows with nans here since many vertebrae not found by AI\n",
    "#Here we don't use 'dropna()' since some attributes/columns might not be present in most measurements but we want to keep the rest\n",
    "df_vertebra.reset_index(drop=True,inplace=True) #reset index\n",
    "\n",
    "df_vertebra.participant_id=df_vertebra.participant_id.astype(int) #Convert first column with participant IDs to integers\n",
    "\n",
    "assert len(np.unique(df_vertebra.participant_id))==len(df_vertebra.participant_id) #Confirm that we only have unique patients in df\n",
    "\n",
    "df_vertebra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "9c88a82d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_vertebra.to_csv('vertebra_RedCap_21-4.csv',index=False) #Save file to csv\n",
    "df_vertebra.to_excel('vertebra_RedCap_21-4.xlsx',index=False) #Save file to excel"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "7c98e08a",
   "metadata": {},
   "source": [
    "### Cardiac Fat information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f55763b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cardiac_fat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7739506",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cardiac_fat=df_cardiac_fat.dropna() #To delete all rows with nans - First row created when adding empty series above\n",
    "df_cardiac_fat.reset_index(drop=True,inplace=True) #reset index\n",
    "\n",
    "df_cardiac_fat.participant_id=df_cardiac_fat.participant_id.astype(int) #Convert first column with participant IDs to integers\n",
    "\n",
    "assert len(np.unique(df_cardiac_fat.participant_id))==len(df_cardiac_fat.participant_id) #Confirm that we only have unique patients in df\n",
    "\n",
    "df_cardiac_fat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "c8a60950",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cardiac_fat.to_csv('cardiac_fat_RedCap_21-4.csv',index=False) #Save file to csv\n",
    "df_cardiac_fat.to_excel('cardiac_fat_RedCap_21-4.xlsx',index=False) #Save file to excel"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "7097a583",
   "metadata": {},
   "source": [
    "### Emphysema information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4fe82af",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_emph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4e6a9aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_emph=df_emph.dropna() #To delete all rows with nans - First row created when adding empty series above\n",
    "df_emph.reset_index(drop=True,inplace=True) #reset index and drop index column\n",
    "df_emph.participant_id=df_emph.participant_id.astype(int) #Convert first column with participant IDs to integers\n",
    "df_emph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "490a9da0",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert len(np.unique(df_emph.participant_id))==len(df_emph.participant_id) #Confirm that we only have unique patients in df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "863e0167",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_emph.to_csv('emph_RedCap_21-4.csv',index=False) #Save file to csv\n",
    "df_emph.to_excel('emph_RedCap_21-4.xlsx',index=False) #Save file to excel"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "77214583",
   "metadata": {},
   "source": [
    "### Nodules information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "055be515",
   "metadata": {},
   "outputs": [],
   "source": [
    "#just for debugging\n",
    "# df_nodules[df_nodules.participant_id==101191] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36a1bbe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_nodules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84916637",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_nodules=df_nodules.iloc[1:] #Remove first row with nan created above\n",
    "#There are many more rows with nans here since we may not have any nodules detected by AI\n",
    "#Here we don't use 'dropna()' since some attributes/columns might not be present (eg. when no nodules detected by AI) but we want to keep that information\n",
    "df_nodules.reset_index(drop=True,inplace=True) #reset index and drop index column\n",
    "df_nodules.participant_id=df_nodules.participant_id.astype(int) #Convert first column with participant IDs to integers\n",
    "df_nodules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "e6ffd5b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert len(np.unique(df_nodules.participant_id))==len(df_nodules.participant_id) #Confirm that we only have unique patients in df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "06aa9664",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Export to REDCap - not the last 10 columns with slice information\n",
    "\n",
    "df_nodules.iloc[:,:-10].to_csv('nodules_to_REDCap_21-4.csv',index=False) #Save file to csv\n",
    "df_nodules.iloc[:,:-10].to_excel('nodules_to_REDCap_21-4.xlsx',index=False) #Save file to xlsx"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "fc3eb71f",
   "metadata": {},
   "source": [
    "### Nodules for BMI experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "f3e92492",
   "metadata": {},
   "outputs": [],
   "source": [
    "# BMI_pats=[] #Initialize empty list to be filled with BMI participant_ids\n",
    "\n",
    "# #Loop over possible directories with BMI participants and add them to the above list\n",
    "# for BMI_pat in os.listdir(os.getcwd()+\"\\BMI_exp\\BMI_high_scans_new\"):\n",
    "#     BMI_pats.append(int(BMI_pat))\n",
    "# for BMI_pat in os.listdir(os.getcwd()+\"\\BMI_exp\\BMI_low_scans_new\"):\n",
    "#     BMI_pats.append(int(BMI_pat))\n",
    "# for BMI_pat in os.listdir(os.getcwd()+\"\\BMI_exp\\BMI_high_scans\"):\n",
    "#     BMI_pats.append(int(BMI_pat))\n",
    "# for BMI_pat in os.listdir(os.getcwd()+\"\\BMI_exp\\BMI_low_scans\"):\n",
    "#     BMI_pats.append(int(BMI_pat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "3f4a8b55",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# df_BMI=df_nodules.loc[df_nodules['participant_id'].isin(BMI_pats)] #Select from all participants with nodules only those of BMI experiment\n",
    "# df_BMI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "ace979cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Check if there is any participant in our experiment that was not selected - If so, might need to resend to AI\n",
    "# for participant_BMI in BMI_pats:\n",
    "#     if participant_BMI not in df_BMI['participant_id'].values:\n",
    "#         print(\"Participant\",participant_BMI,\"not found\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "aa58317d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_BMI.iloc[:,:-10].to_excel('BMI_exp_AI_13-1.xlsx',index=False) #Save file to xlsx - Ignore last 10 columns with slice information\n",
    "#!!WHY IGNORE THE LAST 10? POSITION NEEDED TO CONVERT TO SLICE NUMBER!!! \n",
    "#The actual dataframe needed by the automated algorithm to work is extracted below"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "32062755",
   "metadata": {},
   "source": [
    "### Nodules for Emphysema Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "5629c9b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of individuals with advanced emphysema is 5\n",
      "Number of individuals with confluent emphysema is 7\n",
      "Number of individuals with moderate emphysema is 27\n",
      "Number of individuals with no-emphysema is 82\n"
     ]
    }
   ],
   "source": [
    "#Patient IDs of individuals with advanced, moderate, and noemphysema\n",
    "adv=[163557, 197239, 512145, 670208, 998310] \n",
    "\n",
    "mod=[136550, 136581, 200637, 215387, 240819, 255903, 283229, 294019, 331182, 332758, 438820, 503788, 507704, 609065,\n",
    "     633549, 640431, 660928, 757591, 810826, 811041, 860079,  873698, 971099, 985215, 991277, 101191, 944714]\n",
    "\n",
    "conf=[866164, 282528, 370941, 617769, 754238, 845594,552612] #592863 - has only findings <30 and >300 mm3\n",
    "\n",
    "noemph=[136154, 184429, 295789, 335382, 341417, 353491, 369762, 370347, 382098, 383275, 384136, 395464, 406668, 410655, \n",
    "        427498, 429789, 435703, 440453, 451989, 452500, 493907, 537519, 570103, 591162, 789586, 808262, 146007, 248597, \n",
    "        388787, 428859, 449790, 475503, 485925, 585377, 632817, 673634, 817358, 135915, 136470,  225858, 225969,\n",
    "        278319, 320656, 425409, 490144, 499832, 518709, 582854, 663854, 706029, 870199, 910698, 986374, 988394,\n",
    "        662368, 199391, 427158, 429703, 458362, 545508, 720754, 845334, 891238, 951248, 100785, 113137, 135984, 136012, \n",
    "        136109, 136116, 136185, 136307, 136321,162158, 136418,136432,136456,136487,136494,136425,138310 ,144629]\n",
    "\n",
    "print(\"Number of individuals with advanced emphysema is {}\".format(len(adv))) #5 in total\n",
    "print(\"Number of individuals with confluent emphysema is {}\".format(len(conf))) #7 in total\n",
    "print(\"Number of individuals with moderate emphysema is {}\".format(len(mod))) #27 in total\n",
    "print(\"Number of individuals with no-emphysema is {}\".format(len(noemph))) #82 in total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6241ea36",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_only_nodules=df_nodules.loc[df_nodules['participant_id'].isin(adv+mod+conf+noemph)] #Select only participants of emphysema experiment\n",
    "df_only_nodules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "0edb24d1",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>participant_id</th>\n",
       "      <th>ai_nod_id1</th>\n",
       "      <th>ai_nod_id2</th>\n",
       "      <th>ai_nod_id3</th>\n",
       "      <th>ai_nod_id4</th>\n",
       "      <th>ai_nod_id5</th>\n",
       "      <th>ai_nod_id6</th>\n",
       "      <th>ai_nod_id7</th>\n",
       "      <th>ai_nod_id8</th>\n",
       "      <th>ai_nod_id9</th>\n",
       "      <th>...</th>\n",
       "      <th>ai_nod_dia3d_n1</th>\n",
       "      <th>ai_nod_dia3d_n2</th>\n",
       "      <th>ai_nod_dia3d_n3</th>\n",
       "      <th>ai_nod_dia3d_n4</th>\n",
       "      <th>ai_nod_dia3d_n5</th>\n",
       "      <th>ai_nod_dia3d_n6</th>\n",
       "      <th>ai_nod_dia3d_n7</th>\n",
       "      <th>ai_nod_dia3d_n8</th>\n",
       "      <th>ai_nod_dia3d_n9</th>\n",
       "      <th>ai_nod_dia3d_n10</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>609065</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows × 41 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   participant_id ai_nod_id1 ai_nod_id2 ai_nod_id3 ai_nod_id4 ai_nod_id5  \\\n",
       "0          609065        NaN        NaN        NaN        NaN        NaN   \n",
       "\n",
       "  ai_nod_id6 ai_nod_id7 ai_nod_id8 ai_nod_id9  ... ai_nod_dia3d_n1  \\\n",
       "0        NaN        NaN        NaN        NaN  ...             NaN   \n",
       "\n",
       "  ai_nod_dia3d_n2 ai_nod_dia3d_n3 ai_nod_dia3d_n4 ai_nod_dia3d_n5  \\\n",
       "0             NaN             NaN             NaN             NaN   \n",
       "\n",
       "  ai_nod_dia3d_n6 ai_nod_dia3d_n7 ai_nod_dia3d_n8 ai_nod_dia3d_n9  \\\n",
       "0             NaN             NaN             NaN             NaN   \n",
       "\n",
       "  ai_nod_dia3d_n10  \n",
       "0              NaN  \n",
       "\n",
       "[1 rows x 41 columns]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#The following are some of the participants in which AI didn't detect any nodules (they may have nodules in REDCap)\n",
    "df_nonods=pd.DataFrame(columns=participant_id+nod_ids+nod_volumes+nod_diam_2d+nod_diam_3d)\n",
    "\n",
    "#Check for which participant there is no AI file - Should be empty since these included above, otherwise error\n",
    "nonods_individuals=[]\n",
    "for participant in adv+mod+conf+noemph:\n",
    "    if len(df_nodules[df_nodules.participant_id==participant])==0:\n",
    "        nonods_individuals.append(participant)\n",
    "\n",
    "df_nonods.participant_id=nonods_individuals\n",
    "df_nonods #Only first column filled - list of participants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71e5df4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#In df_only_nodules we may also have patients with no nodules - Those that were sent to AI only for aorta and not nodules \n",
    "combined=[df_only_nodules,df_nonods]\n",
    "df_nodules_final=pd.concat(combined)\n",
    "df_nodules_final #All participants with nodules and without\n",
    "#Define below if we want to create the file for automated algorithm for those participants"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "03c4f771",
   "metadata": {},
   "source": [
    "#### Below are the actual dataframe needed from AI exports for the automated algorithm to work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1034329e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Specify below which dataframe to export, 'df_BMI' (BMI experiment) or 'df_nodules_final' (emphysema experiment)\n",
    "\n",
    "#Columns needed are those with the volume of nodules and their location\n",
    "df_want=df_nodules_final[['participant_id','ai_nod_vol1','ai_nod_vol2','ai_nod_vol3','ai_nod_vol4',\n",
    "                  'ai_nod_vol5','ai_nod_vol6','ai_nod_vol7','ai_nod_vol8','ai_nod_vol9','ai_nod_vol10',\n",
    "                 'pos1','pos2','pos3','pos4','pos5','pos6','pos7','pos8','pos9','pos10']]\n",
    "\n",
    "df_want=df_want.sort_values(by='participant_id') #Sort by participant_id\n",
    "df_want\n",
    "\n",
    "#Rename column names\n",
    "df_want.set_axis(['participant_id','V 01','V 02','V 03','V 04','V 05','V 06','V 07','V 08','V 09','V 10',\n",
    "                 'L 01','L 02','L 03','L 04','L 05','L 06','L 07','L 08','L 09','L 10'],axis=1,inplace=True)\n",
    "\n",
    "df_want.set_index('participant_id') #Replace index column\n",
    "df_want['num_nodules']=0 #Extra column that will be filled with num on nods AI detects\n",
    "df_want.fillna('-',inplace=True) #replace nan with '-'\n",
    "df_want.replace('','-',inplace=True) #replace empty cells with '-'\n",
    "\n",
    "for i in range(len(df_want)): #Create new column with number of nodules detected\n",
    "    df_want['num_nodules'].iloc[i]=int(10-np.sum(df_want.iloc[i]=='-')/2)\n",
    "    \n",
    "df_want"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "a044e662",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_want.to_excel('emph_exp_AI_21-4.xlsx',index=False) #Final file used by automation algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "43130ff9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It took 171.33628106117249 secs to run\n"
     ]
    }
   ],
   "source": [
    "end=time.time()\n",
    "print(\"It took {} secs to run\".format(end-start)) #~3min "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
